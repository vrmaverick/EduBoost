{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_strokes(strokes, max_length=100):\n",
    "    \"\"\"\n",
    "    Normalize and pad strokes to ensure a consistent input shape.\n",
    "    - Normalizes (X, Y) coordinates.\n",
    "    - Pads/truncates strokes to max_length.\n",
    "    \"\"\"\n",
    "    all_points = np.concatenate(strokes, axis=0) if strokes else np.array([[0, 0]])\n",
    "    \n",
    "    # Normalize X and Y to [0,1] range\n",
    "    min_vals = np.min(all_points, axis=0)\n",
    "    max_vals = np.max(all_points, axis=0)\n",
    "    norm_strokes = [(s - min_vals) / (max_vals - min_vals + 1e-5) for s in strokes]\n",
    "    \n",
    "    # Flatten strokes and pad/truncate to fixed size\n",
    "    flat_strokes = np.concatenate(norm_strokes, axis=0)[:max_length]\n",
    "    pad_length = max_length - len(flat_strokes)\n",
    "    padded_strokes = np.pad(flat_strokes, ((0, pad_length), (0, 0)), mode='constant')\n",
    "    \n",
    "    return padded_strokes\n",
    "import os\n",
    "def load_dataset(inkml_dirs, num_files=1000, max_length=100):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    charset = set()\n",
    "\n",
    "    for root_dir in inkml_dirs:\n",
    "        if num_files == 'None' :\n",
    "            files = [f for f in os.listdir(root_dir) if f.endswith('.inkml')]\n",
    "        else:\n",
    "            files = [f for f in os.listdir(root_dir) if f.endswith('.inkml')][:num_files]  # Limit files\n",
    "            \n",
    "        for file in files:\n",
    "            path = os.path.join(root_dir, file)\n",
    "            strokes, label = parse_inkml(path)\n",
    "            \n",
    "            if strokes and label:\n",
    "                processed_strokes = preprocess_strokes(strokes, max_length)\n",
    "                samples.append(processed_strokes)\n",
    "                labels.append(label)\n",
    "                charset.update(label)\n",
    "\n",
    "    # Create character mappings\n",
    "    char2idx = {c: i + 1 for i, c in enumerate(sorted(charset))}\n",
    "    char2idx['<pad>'] = 0\n",
    "    idx2char = {v: k for k, v in char2idx.items()}\n",
    "\n",
    "    return samples, labels, char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model1 = tf.keras.models.load_model('exp2.h5')\n",
    "model0 = tf.keras.models.load_model('final_model.h5')\n",
    "model2 = tf.keras.models.load_model('exp3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inkml(file_path):\n",
    "    \"\"\"\n",
    "    Parse an InkML file and return:\n",
    "    - strokes: a list of numpy arrays (each is a stroke of shape [n_points, 2])\n",
    "    - label: the LaTeX label (ground truth) if available\n",
    "    \"\"\"\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Handling namespaces (InkML files typically use a default InkML namespace)\n",
    "    ns = {'ink': 'http://www.w3.org/2003/InkML'}\n",
    "    \n",
    "    # Get annotation (if present)\n",
    "    label = None\n",
    "    for annotation in root.findall('ink:annotation', ns):\n",
    "        # Choose normalizedLabel if available; fallback to label\n",
    "        if annotation.get('type') == 'normalizedLabel':\n",
    "            label = annotation.text.strip()\n",
    "            break\n",
    "        elif annotation.get('type') == 'label':\n",
    "            label = annotation.text.strip()\n",
    "    \n",
    "    strokes = []\n",
    "    # Each <trace> element contains a stroke\n",
    "    for trace in root.findall('ink:trace', ns):\n",
    "        trace_data = trace.text.strip()\n",
    "        points = []\n",
    "        # InkML usually separates points by commas and coordinates by space\n",
    "        for point_str in trace_data.split(','):\n",
    "            # Remove extra spaces and split by whitespace\n",
    "            coords = point_str.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                x, y = float(coords[0]), float(coords[1])\n",
    "                points.append([x, y])\n",
    "        if points:\n",
    "            strokes.append(np.array(points))\n",
    "    \n",
    "    return strokes, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7644 samples\n"
     ]
    }
   ],
   "source": [
    "num_test_files = 15000   # Get value from slider\n",
    "test_samples, test_labels, test_char2idx, test_idx2char = load_dataset(['./data/mathwriting-2024/mathwriting-2024/test'], num_files=num_test_files)\n",
    "print(f\"Loaded {len(test_samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strokes (samples) into a padded sequence format\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocess(samples, labels, char2idx):\n",
    "    # max_seq_length = max(len(s) for s in samples)\n",
    "    target_seq_length = 110  # Find the longest stroke sequence\n",
    "    padded_samples = np.array([\n",
    "        np.pad(s, ((0, max(0, target_seq_length - len(s))), (0, 0)), mode='constant')[:target_seq_length]\n",
    "        for s in samples\n",
    "    ])\n",
    "    \n",
    "    # Ensure padded_samples has shape (batch_size, 110, features)\n",
    "    padded_samples = np.pad(padded_samples, ((0, 0), (0, 0), (0, 1)), mode='constant')\n",
    "\n",
    "    # Convert labels into numerical format using char2idx\n",
    "    numerical_labels = [[char2idx[c] for c in label] for label in labels]\n",
    "\n",
    "    # Pad label sequences\n",
    "    max_label_length = max(\n",
    "    max(len(label) for label in labels),\n",
    "    max(len(label) for label in test_labels))\n",
    "    # Pad label sequences\n",
    "    padded_labels = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        numerical_labels, maxlen=max_label_length, padding='post'\n",
    "    )\n",
    "    # padded_labels = tf.keras.preprocessing.sequence.pad_sequences(numerical_labels, padding='post')\n",
    "\n",
    "    # Convert to TensorFlow datasets for efficient loading\n",
    "    # BATCH_SIZE = 32           \n",
    "\n",
    "    # train_dataset = tf.data.Dataset.from_tensor_slices((padded_samples, padded_labels))\n",
    "    # train_dataset = train_dataset.shuffle(10).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    # return train_dataset\n",
    "    return padded_samples, padded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_y = preprocess(test_samples, test_labels, test_char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7644, 110, 3) (7644, 173)\n"
     ]
    }
   ],
   "source": [
    "print(test_X.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "start_token = 0  # Define the start token (adjust based on your dataset)\n",
    "test_decoder_inputs = np.pad(test_y[:, :-1], ((0, 0), (1, 0)), constant_values=start_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert an image to inkml format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_strokes_with_timestamps(image_path):\n",
    "    # Load image in grayscale\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Find contours (these represent strokes)\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    strokes = []\n",
    "    timestamp = 0  # Initialize timestamp\n",
    "    \n",
    "    # Get image dimensions for normalization\n",
    "    h, w = img.shape\n",
    "\n",
    "    for contour in contours:\n",
    "        stroke = []\n",
    "        for point in contour:\n",
    "            x, y = int(point[0][0]), int(point[0][1])\n",
    "            \n",
    "            # Normalize x and y coordinates to [0, 1]\n",
    "            norm_x = round(x / w, 8)\n",
    "            norm_y = round(y / h, 8)\n",
    "            \n",
    "            stroke.append((norm_x, norm_y, timestamp))\n",
    "            timestamp += 1  # Increment timestamp for each point\n",
    "        strokes.append(stroke)\n",
    "    \n",
    "    return strokes\n",
    "\n",
    "def strokes_to_inkml(strokes, output_file=\"output.inkml\"):\n",
    "    root = ET.Element(\"ink\", xmlns=\"http://www.w3.org/2003/InkML\")\n",
    "    trace_group = ET.SubElement(root, \"traceGroup\")\n",
    "    \n",
    "    for i, stroke in enumerate(strokes):\n",
    "        trace = ET.SubElement(trace_group, \"trace\", id=str(i))\n",
    "        trace.text = \" \".join(f\"{x:.4f},{y:.4f},{t}\" for x, y, t in stroke)\n",
    "    \n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write(output_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "    print(f\"InkML saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.94021739, 0.40111421, 0), (0.94202899, 0.39832869, 1), (0.94384058, 0.39832869, 2), (0.94565217, 0.39832869, 3), (0.94746377, 0.39832869, 4), (0.94927536, 0.39832869, 5), (0.95108696, 0.39832869, 6), (0.95289855, 0.40111421, 7), (0.95289855, 0.40389972, 8), (0.95108696, 0.40668524, 9), (0.94927536, 0.40668524, 10), (0.94746377, 0.40668524, 11), (0.94565217, 0.40668524, 12), (0.94384058, 0.40668524, 13), (0.94202899, 0.40947075, 14), (0.94021739, 0.40947075, 15), (0.9384058, 0.40947075, 16), (0.9365942, 0.41225627, 17), (0.93478261, 0.41225627, 18), (0.93297101, 0.41225627, 19), (0.93115942, 0.41225627, 20), (0.92934783, 0.41225627, 21), (0.92753623, 0.41225627, 22), (0.92572464, 0.41225627, 23), (0.92391304, 0.41225627, 24), (0.92210145, 0.41225627, 25), (0.92028986, 0.41225627, 26), (0.91847826, 0.41225627, 27), (0.91666667, 0.41504178, 28), (0.91485507, 0.41504178, 29), (0.91304348, 0.41504178, 30), (0.91123188, 0.4178273, 31), (0.90942029, 0.4178273, 32), (0.9076087, 0.4178273, 33), (0.9057971, 0.4178273, 34), (0.90398551, 0.4178273, 35), (0.90217391, 0.4178273, 36), (0.90036232, 0.42061281, 37), (0.89855072, 0.42061281, 38), (0.89673913, 0.42061281, 39), (0.89492754, 0.42061281, 40), (0.89311594, 0.42061281, 41), (0.89130435, 0.42061281, 42), (0.88949275, 0.42061281, 43), (0.88768116, 0.42339833, 44), (0.88586957, 0.42339833, 45), (0.88405797, 0.42339833, 46), (0.88224638, 0.42339833, 47), (0.88043478, 0.42339833, 48), (0.87862319, 0.42618384, 49), (0.87681159, 0.42618384, 50), (0.875, 0.42618384, 51), (0.87318841, 0.42618384, 52), (0.87137681, 0.42896936, 53), (0.86956522, 0.42896936, 54), (0.86775362, 0.42896936, 55), (0.86594203, 0.43175487, 56), (0.86413043, 0.43175487, 57), (0.86231884, 0.43175487, 58), (0.86050725, 0.43175487, 59), (0.85869565, 0.43175487, 60), (0.85688406, 0.43175487, 61), (0.85507246, 0.43175487, 62), (0.85326087, 0.43175487, 63), (0.85144928, 0.43175487, 64), (0.84963768, 0.43175487, 65), (0.84782609, 0.43175487, 66), (0.84601449, 0.43175487, 67), (0.8442029, 0.43175487, 68), (0.8423913, 0.43454039, 69), (0.84057971, 0.43454039, 70), (0.83876812, 0.43454039, 71), (0.83695652, 0.43454039, 72), (0.83514493, 0.43454039, 73), (0.83333333, 0.43454039, 74), (0.83152174, 0.43454039, 75), (0.82971014, 0.43454039, 76), (0.82789855, 0.43454039, 77), (0.82608696, 0.43732591, 78), (0.82427536, 0.43732591, 79), (0.82246377, 0.43732591, 80), (0.82065217, 0.43732591, 81), (0.81884058, 0.43732591, 82), (0.81702899, 0.43732591, 83), (0.81521739, 0.43732591, 84), (0.8134058, 0.44011142, 85), (0.8115942, 0.44011142, 86), (0.80978261, 0.44011142, 87), (0.80797101, 0.44011142, 88), (0.80615942, 0.44011142, 89), (0.80434783, 0.44011142, 90), (0.80253623, 0.44289694, 91), (0.80072464, 0.44289694, 92), (0.79891304, 0.44289694, 93), (0.79710145, 0.44289694, 94), (0.79528986, 0.44289694, 95), (0.79347826, 0.44289694, 96), (0.79166667, 0.44289694, 97), (0.78985507, 0.44289694, 98), (0.78804348, 0.44011142, 99), (0.78804348, 0.43732591, 100), (0.78985507, 0.43454039, 101), (0.79166667, 0.43454039, 102), (0.79347826, 0.43454039, 103), (0.79528986, 0.43454039, 104), (0.79710145, 0.43454039, 105), (0.79891304, 0.43454039, 106), (0.80072464, 0.43454039, 107), (0.80253623, 0.43175487, 108), (0.80434783, 0.43175487, 109), (0.80615942, 0.43175487, 110), (0.80797101, 0.43175487, 111), (0.80978261, 0.43175487, 112), (0.8115942, 0.43175487, 113), (0.8134058, 0.43175487, 114), (0.81521739, 0.42896936, 115), (0.81702899, 0.42896936, 116), (0.81884058, 0.42896936, 117), (0.82065217, 0.42896936, 118), (0.82246377, 0.42896936, 119), (0.82427536, 0.42896936, 120), (0.82608696, 0.42618384, 121), (0.82789855, 0.42618384, 122), (0.82971014, 0.42618384, 123), (0.83152174, 0.42618384, 124), (0.83333333, 0.42618384, 125), (0.83514493, 0.42618384, 126), (0.83695652, 0.42618384, 127), (0.83876812, 0.42618384, 128), (0.84057971, 0.42618384, 129), (0.8423913, 0.42339833, 130), (0.8442029, 0.42339833, 131), (0.84601449, 0.42339833, 132), (0.84782609, 0.42339833, 133), (0.84963768, 0.42339833, 134), (0.85144928, 0.42339833, 135), (0.85326087, 0.42339833, 136), (0.85507246, 0.42339833, 137), (0.85688406, 0.42339833, 138), (0.85869565, 0.42339833, 139), (0.86050725, 0.42339833, 140), (0.86231884, 0.42339833, 141), (0.86413043, 0.42339833, 142), (0.86594203, 0.42339833, 143), (0.86775362, 0.42061281, 144), (0.86956522, 0.42061281, 145), (0.87137681, 0.4178273, 146), (0.87318841, 0.4178273, 147), (0.875, 0.4178273, 148), (0.87681159, 0.4178273, 149), (0.87862319, 0.41504178, 150), (0.88043478, 0.41504178, 151), (0.88224638, 0.41504178, 152), (0.88405797, 0.41504178, 153), (0.88586957, 0.41504178, 154), (0.88768116, 0.41504178, 155), (0.88949275, 0.41225627, 156), (0.89130435, 0.41225627, 157), (0.89311594, 0.41225627, 158), (0.89492754, 0.41225627, 159), (0.89673913, 0.41225627, 160), (0.89855072, 0.41225627, 161), (0.90036232, 0.41225627, 162), (0.90217391, 0.40947075, 163), (0.90398551, 0.40947075, 164), (0.9057971, 0.40947075, 165), (0.9076087, 0.40947075, 166), (0.90942029, 0.40947075, 167), (0.91123188, 0.40668524, 168), (0.91304348, 0.40668524, 169), (0.91485507, 0.40668524, 170), (0.91666667, 0.40389972, 171), (0.91847826, 0.40389972, 172), (0.92028986, 0.40389972, 173), (0.92210145, 0.40389972, 174), (0.92391304, 0.40389972, 175), (0.92572464, 0.40389972, 176), (0.92753623, 0.40389972, 177), (0.92934783, 0.40389972, 178), (0.93115942, 0.40389972, 179), (0.93297101, 0.40389972, 180), (0.93478261, 0.40389972, 181), (0.9365942, 0.40111421, 182), (0.9384058, 0.40111421, 183)], [(0.28985507, 0.31754875, 184), (0.29166667, 0.31476323, 185), (0.29347826, 0.31476323, 186), (0.29528986, 0.31754875, 187), (0.29528986, 0.32033426, 188), (0.29528986, 0.32311978, 189), (0.29528986, 0.32590529, 190), (0.29528986, 0.32869081, 191), (0.29528986, 0.33147632, 192), (0.29710145, 0.33426184, 193), (0.29710145, 0.33704735, 194), (0.29710145, 0.33983287, 195), (0.29710145, 0.34261838, 196), (0.29891304, 0.3454039, 197), (0.29891304, 0.34818942, 198), (0.29891304, 0.35097493, 199), (0.29891304, 0.35376045, 200), (0.30072464, 0.35654596, 201), (0.30072464, 0.35933148, 202), (0.30072464, 0.36211699, 203), (0.30072464, 0.36490251, 204), (0.30253623, 0.36768802, 205), (0.30253623, 0.37047354, 206), (0.30253623, 0.37325905, 207), (0.30253623, 0.37604457, 208), (0.30434783, 0.37883008, 209), (0.30434783, 0.3816156, 210), (0.30434783, 0.38440111, 211), (0.30615942, 0.38718663, 212), (0.30615942, 0.38997214, 213), (0.30615942, 0.39275766, 214), (0.30615942, 0.39554318, 215), (0.30797101, 0.39832869, 216), (0.30797101, 0.40111421, 217), (0.30797101, 0.40389972, 218), (0.30797101, 0.40668524, 219), (0.30797101, 0.40947075, 220), (0.30797101, 0.41225627, 221), (0.30797101, 0.41504178, 222), (0.30978261, 0.4178273, 223), (0.30978261, 0.42061281, 224), (0.30978261, 0.42339833, 225), (0.30978261, 0.42618384, 226), (0.30978261, 0.42896936, 227), (0.30978261, 0.43175487, 228), (0.30978261, 0.43454039, 229), (0.3115942, 0.43732591, 230), (0.3134058, 0.43732591, 231), (0.31521739, 0.43454039, 232), (0.31702899, 0.43454039, 233), (0.31884058, 0.43454039, 234), (0.32065217, 0.43454039, 235), (0.32246377, 0.43454039, 236), (0.32427536, 0.43454039, 237), (0.32608696, 0.43175487, 238), (0.32789855, 0.43175487, 239), (0.32971014, 0.43175487, 240), (0.33152174, 0.43175487, 241), (0.33333333, 0.43175487, 242), (0.33514493, 0.43175487, 243), (0.33695652, 0.43175487, 244), (0.33876812, 0.43175487, 245), (0.34057971, 0.43175487, 246), (0.3423913, 0.42896936, 247), (0.3442029, 0.42896936, 248), (0.34601449, 0.42896936, 249), (0.34782609, 0.42896936, 250), (0.34963768, 0.42896936, 251), (0.35144928, 0.42896936, 252), (0.35326087, 0.42896936, 253), (0.35507246, 0.42896936, 254), (0.35688406, 0.42618384, 255), (0.35869565, 0.42618384, 256), (0.36050725, 0.42618384, 257), (0.36231884, 0.42618384, 258), (0.36413043, 0.42618384, 259), (0.36594203, 0.42618384, 260), (0.36775362, 0.42618384, 261), (0.36956522, 0.42618384, 262), (0.37137681, 0.42339833, 263), (0.37318841, 0.42339833, 264), (0.375, 0.42339833, 265), (0.37681159, 0.42339833, 266), (0.37862319, 0.42339833, 267), (0.38043478, 0.42339833, 268), (0.38224638, 0.42339833, 269), (0.38405797, 0.42339833, 270), (0.38586957, 0.42061281, 271), (0.38768116, 0.42061281, 272), (0.38949275, 0.42061281, 273), (0.39130435, 0.42061281, 274), (0.39311594, 0.42061281, 275), (0.39492754, 0.42061281, 276), (0.39673913, 0.42061281, 277), (0.39855072, 0.42061281, 278), (0.40036232, 0.42061281, 279), (0.40217391, 0.42061281, 280), (0.40398551, 0.42061281, 281), (0.4057971, 0.4178273, 282), (0.4076087, 0.4178273, 283), (0.40942029, 0.4178273, 284), (0.41123188, 0.4178273, 285), (0.41304348, 0.4178273, 286), (0.41485507, 0.4178273, 287), (0.41666667, 0.4178273, 288), (0.41847826, 0.4178273, 289), (0.42028986, 0.4178273, 290), (0.42210145, 0.4178273, 291), (0.42391304, 0.41504178, 292), (0.42572464, 0.41504178, 293), (0.42753623, 0.41504178, 294), (0.42934783, 0.41504178, 295), (0.43115942, 0.41504178, 296), (0.43297101, 0.41225627, 297), (0.43478261, 0.41225627, 298), (0.4365942, 0.41225627, 299), (0.4384058, 0.41225627, 300), (0.44021739, 0.41225627, 301), (0.44202899, 0.41225627, 302), (0.44384058, 0.41225627, 303), (0.44565217, 0.40947075, 304), (0.44746377, 0.40947075, 305), (0.44927536, 0.40947075, 306), (0.45108696, 0.40947075, 307), (0.45289855, 0.40947075, 308), (0.45471014, 0.40947075, 309), (0.45652174, 0.40947075, 310), (0.45833333, 0.40947075, 311), (0.46014493, 0.40947075, 312), (0.46195652, 0.40947075, 313), (0.46376812, 0.40947075, 314), (0.46557971, 0.40668524, 315), (0.4673913, 0.40668524, 316), (0.4692029, 0.40668524, 317), (0.47101449, 0.40668524, 318), (0.47282609, 0.40947075, 319), (0.47282609, 0.41225627, 320), (0.47101449, 0.41504178, 321), (0.4692029, 0.41504178, 322), (0.4673913, 0.41504178, 323), (0.46557971, 0.4178273, 324), (0.46376812, 0.4178273, 325), (0.46195652, 0.4178273, 326), (0.46014493, 0.4178273, 327), (0.45833333, 0.4178273, 328), (0.45652174, 0.4178273, 329), (0.45471014, 0.4178273, 330), (0.45289855, 0.4178273, 331), (0.45108696, 0.4178273, 332), (0.44927536, 0.4178273, 333), (0.44746377, 0.4178273, 334), (0.44565217, 0.4178273, 335), (0.44384058, 0.42061281, 336), (0.44202899, 0.42061281, 337), (0.44021739, 0.42061281, 338), (0.4384058, 0.42061281, 339), (0.4365942, 0.42061281, 340), (0.43478261, 0.42061281, 341), (0.43297101, 0.42061281, 342), (0.43115942, 0.42339833, 343), (0.42934783, 0.42339833, 344), (0.42753623, 0.42339833, 345), (0.42572464, 0.42339833, 346), (0.42391304, 0.42339833, 347), (0.42210145, 0.42618384, 348), (0.42028986, 0.42618384, 349), (0.41847826, 0.42618384, 350), (0.41666667, 0.42618384, 351), (0.41485507, 0.42618384, 352), (0.41304348, 0.42618384, 353), (0.41123188, 0.42618384, 354), (0.40942029, 0.42618384, 355), (0.4076087, 0.42618384, 356), (0.4057971, 0.42618384, 357), (0.40398551, 0.42896936, 358), (0.40217391, 0.42896936, 359), (0.40036232, 0.42896936, 360), (0.39855072, 0.42896936, 361), (0.39673913, 0.42896936, 362), (0.39492754, 0.42896936, 363), (0.39311594, 0.42896936, 364), (0.39130435, 0.42896936, 365), (0.38949275, 0.42896936, 366), (0.38768116, 0.42896936, 367), (0.38586957, 0.42896936, 368), (0.38405797, 0.43175487, 369), (0.38224638, 0.43175487, 370), (0.38043478, 0.43175487, 371), (0.37862319, 0.43175487, 372), (0.37681159, 0.43175487, 373), (0.375, 0.43175487, 374), (0.37318841, 0.43175487, 375), (0.37137681, 0.43175487, 376), (0.36956522, 0.43454039, 377), (0.36775362, 0.43454039, 378), (0.36594203, 0.43454039, 379), (0.36413043, 0.43454039, 380), (0.36231884, 0.43454039, 381), (0.36050725, 0.43454039, 382), (0.35869565, 0.43454039, 383), (0.35688406, 0.43454039, 384), (0.35507246, 0.43732591, 385), (0.35326087, 0.43732591, 386), (0.35144928, 0.43732591, 387), (0.34963768, 0.43732591, 388), (0.34782609, 0.43732591, 389), (0.34601449, 0.43732591, 390), (0.3442029, 0.43732591, 391), (0.3423913, 0.43732591, 392), (0.34057971, 0.44011142, 393), (0.33876812, 0.44011142, 394), (0.33695652, 0.44011142, 395), (0.33514493, 0.44011142, 396), (0.33333333, 0.44011142, 397), (0.33152174, 0.44011142, 398), (0.32971014, 0.44011142, 399), (0.32789855, 0.44011142, 400), (0.32608696, 0.44011142, 401), (0.32427536, 0.44289694, 402), (0.32246377, 0.44289694, 403), (0.32065217, 0.44289694, 404), (0.31884058, 0.44289694, 405), (0.31702899, 0.44289694, 406), (0.31521739, 0.44289694, 407), (0.3134058, 0.44568245, 408), (0.3134058, 0.44846797, 409), (0.31521739, 0.45125348, 410), (0.31521739, 0.454039, 411), (0.31521739, 0.45682451, 412), (0.31521739, 0.45961003, 413), (0.31521739, 0.46239554, 414), (0.31702899, 0.46518106, 415), (0.31702899, 0.46796657, 416), (0.31702899, 0.47075209, 417), (0.31702899, 0.4735376, 418), (0.31702899, 0.47632312, 419), (0.31884058, 0.47910864, 420), (0.31884058, 0.48189415, 421), (0.31884058, 0.48467967, 422), (0.31884058, 0.48746518, 423), (0.31884058, 0.4902507, 424), (0.31884058, 0.49303621, 425), (0.32065217, 0.49582173, 426), (0.32065217, 0.49860724, 427), (0.32065217, 0.50139276, 428), (0.32246377, 0.50417827, 429), (0.32246377, 0.50696379, 430), (0.32246377, 0.5097493, 431), (0.32246377, 0.51253482, 432), (0.32246377, 0.51532033, 433), (0.32246377, 0.51810585, 434), (0.32246377, 0.52089136, 435), (0.32065217, 0.52367688, 436), (0.31884058, 0.52367688, 437), (0.31702899, 0.52089136, 438), (0.31702899, 0.51810585, 439), (0.31702899, 0.51532033, 440), (0.31702899, 0.51253482, 441), (0.31702899, 0.5097493, 442), (0.31702899, 0.50696379, 443), (0.31521739, 0.50417827, 444), (0.31521739, 0.50139276, 445), (0.31521739, 0.49860724, 446), (0.3134058, 0.49582173, 447), (0.3134058, 0.49303621, 448), (0.3134058, 0.4902507, 449), (0.3134058, 0.48746518, 450), (0.3134058, 0.48467967, 451), (0.3134058, 0.48189415, 452), (0.3134058, 0.47910864, 453), (0.3115942, 0.47632312, 454), (0.3115942, 0.4735376, 455), (0.3115942, 0.47075209, 456), (0.3115942, 0.46796657, 457), (0.3115942, 0.46518106, 458), (0.30978261, 0.46239554, 459), (0.30978261, 0.45961003, 460), (0.30978261, 0.45682451, 461), (0.30978261, 0.454039, 462), (0.30797101, 0.45125348, 463), (0.30797101, 0.44846797, 464), (0.30797101, 0.44568245, 465), (0.30615942, 0.44568245, 466), (0.30434783, 0.44846797, 467), (0.30253623, 0.44846797, 468), (0.30072464, 0.44846797, 469), (0.29891304, 0.44568245, 470), (0.29891304, 0.44289694, 471), (0.30072464, 0.44011142, 472), (0.30253623, 0.44011142, 473), (0.30434783, 0.44011142, 474), (0.30434783, 0.43732591, 475), (0.30434783, 0.43454039, 476), (0.30434783, 0.43175487, 477), (0.30434783, 0.42896936, 478), (0.30434783, 0.42618384, 479), (0.30434783, 0.42339833, 480), (0.30434783, 0.42061281, 481), (0.30253623, 0.4178273, 482), (0.30253623, 0.41504178, 483), (0.30253623, 0.41225627, 484), (0.30253623, 0.40947075, 485), (0.30253623, 0.40668524, 486), (0.30253623, 0.40389972, 487), (0.30253623, 0.40111421, 488), (0.30253623, 0.39832869, 489), (0.30072464, 0.39554318, 490), (0.30072464, 0.39275766, 491), (0.30072464, 0.38997214, 492), (0.29891304, 0.38718663, 493), (0.29891304, 0.38440111, 494), (0.29891304, 0.3816156, 495), (0.29891304, 0.37883008, 496), (0.29710145, 0.37604457, 497), (0.29710145, 0.37325905, 498), (0.29710145, 0.37047354, 499), (0.29710145, 0.36768802, 500), (0.29528986, 0.36490251, 501), (0.29528986, 0.36211699, 502), (0.29528986, 0.35933148, 503), (0.29528986, 0.35654596, 504), (0.29347826, 0.35376045, 505), (0.29347826, 0.35097493, 506), (0.29347826, 0.34818942, 507), (0.29347826, 0.3454039, 508), (0.29166667, 0.34261838, 509), (0.29166667, 0.33983287, 510), (0.29166667, 0.33704735, 511), (0.29166667, 0.33426184, 512), (0.28985507, 0.33147632, 513), (0.28985507, 0.32869081, 514), (0.28985507, 0.32590529, 515), (0.28985507, 0.32311978, 516), (0.28985507, 0.32033426, 517)], [(0.88405797, 0.29526462, 518), (0.88586957, 0.29247911, 519), (0.88768116, 0.29247911, 520), (0.88949275, 0.29247911, 521), (0.89130435, 0.29247911, 522), (0.89311594, 0.29247911, 523), (0.89492754, 0.29247911, 524), (0.89673913, 0.29526462, 525), (0.89673913, 0.29805014, 526), (0.89492754, 0.30083565, 527), (0.89311594, 0.30083565, 528), (0.89130435, 0.30083565, 529), (0.88949275, 0.30083565, 530), (0.88768116, 0.30083565, 531), (0.88586957, 0.30362117, 532), (0.88405797, 0.30362117, 533), (0.88224638, 0.30362117, 534), (0.88043478, 0.30362117, 535), (0.87862319, 0.30362117, 536), (0.87681159, 0.30362117, 537), (0.875, 0.30362117, 538), (0.87318841, 0.30640669, 539), (0.87137681, 0.30640669, 540), (0.86956522, 0.30640669, 541), (0.86775362, 0.30640669, 542), (0.86594203, 0.30640669, 543), (0.86413043, 0.30640669, 544), (0.86231884, 0.3091922, 545), (0.86050725, 0.3091922, 546), (0.85869565, 0.3091922, 547), (0.85688406, 0.3091922, 548), (0.85507246, 0.3091922, 549), (0.85326087, 0.3091922, 550), (0.85144928, 0.3091922, 551), (0.84963768, 0.3091922, 552), (0.84782609, 0.31197772, 553), (0.84601449, 0.31197772, 554), (0.8442029, 0.31197772, 555), (0.8423913, 0.31197772, 556), (0.84057971, 0.31197772, 557), (0.83876812, 0.31197772, 558), (0.83695652, 0.31197772, 559), (0.83514493, 0.31197772, 560), (0.83333333, 0.31197772, 561), (0.83152174, 0.31476323, 562), (0.82971014, 0.31476323, 563), (0.82789855, 0.31476323, 564), (0.82608696, 0.31476323, 565), (0.82427536, 0.31476323, 566), (0.82246377, 0.31476323, 567), (0.82065217, 0.31476323, 568), (0.81884058, 0.31476323, 569), (0.81702899, 0.31476323, 570), (0.81521739, 0.31476323, 571), (0.8134058, 0.31754875, 572), (0.8115942, 0.31754875, 573), (0.80978261, 0.31754875, 574), (0.80797101, 0.31754875, 575), (0.80615942, 0.31754875, 576), (0.80434783, 0.31754875, 577), (0.80253623, 0.31754875, 578), (0.80072464, 0.31754875, 579), (0.79891304, 0.31754875, 580), (0.79710145, 0.31754875, 581), (0.79528986, 0.31754875, 582), (0.79347826, 0.32033426, 583), (0.79166667, 0.32033426, 584), (0.78985507, 0.32033426, 585), (0.78804348, 0.32033426, 586), (0.78623188, 0.32033426, 587), (0.78442029, 0.32033426, 588), (0.7826087, 0.32033426, 589), (0.7807971, 0.32033426, 590), (0.77898551, 0.32033426, 591), (0.77717391, 0.32311978, 592), (0.77536232, 0.32311978, 593), (0.77355072, 0.32311978, 594), (0.77173913, 0.32311978, 595), (0.76992754, 0.32311978, 596), (0.76811594, 0.32311978, 597), (0.76630435, 0.32590529, 598), (0.76449275, 0.32590529, 599), (0.76268116, 0.32590529, 600), (0.76086957, 0.32590529, 601), (0.75905797, 0.32590529, 602), (0.75724638, 0.32590529, 603), (0.75543478, 0.32590529, 604), (0.75362319, 0.32869081, 605), (0.75181159, 0.32869081, 606), (0.75, 0.32869081, 607), (0.74818841, 0.32869081, 608), (0.74637681, 0.32869081, 609), (0.74456522, 0.32869081, 610), (0.74275362, 0.32869081, 611), (0.74094203, 0.32869081, 612), (0.73913043, 0.32869081, 613), (0.73731884, 0.32590529, 614), (0.73731884, 0.32311978, 615), (0.73913043, 0.32033426, 616), (0.74094203, 0.32033426, 617), (0.74275362, 0.32033426, 618), (0.74456522, 0.32033426, 619), (0.74637681, 0.32033426, 620), (0.74818841, 0.32033426, 621), (0.75, 0.32033426, 622), (0.75181159, 0.32033426, 623), (0.75362319, 0.32033426, 624), (0.75543478, 0.31754875, 625), (0.75724638, 0.31754875, 626), (0.75905797, 0.31754875, 627), (0.76086957, 0.31754875, 628), (0.76268116, 0.31754875, 629), (0.76449275, 0.31754875, 630), (0.76630435, 0.31754875, 631), (0.76811594, 0.31476323, 632), (0.76992754, 0.31476323, 633), (0.77173913, 0.31476323, 634), (0.77355072, 0.31476323, 635), (0.77536232, 0.31476323, 636), (0.77717391, 0.31476323, 637), (0.77898551, 0.31197772, 638), (0.7807971, 0.31197772, 639), (0.7826087, 0.31197772, 640), (0.78442029, 0.31197772, 641), (0.78623188, 0.31197772, 642), (0.78804348, 0.31197772, 643), (0.78985507, 0.31197772, 644), (0.79166667, 0.31197772, 645), (0.79347826, 0.3091922, 646), (0.79528986, 0.3091922, 647), (0.79710145, 0.3091922, 648), (0.79891304, 0.3091922, 649), (0.80072464, 0.3091922, 650), (0.80253623, 0.3091922, 651), (0.80434783, 0.3091922, 652), (0.80615942, 0.3091922, 653), (0.80797101, 0.3091922, 654), (0.80978261, 0.3091922, 655), (0.8115942, 0.3091922, 656), (0.8134058, 0.30640669, 657), (0.81521739, 0.30640669, 658), (0.81702899, 0.30640669, 659), (0.81884058, 0.30640669, 660), (0.82065217, 0.30640669, 661), (0.82246377, 0.30640669, 662), (0.82427536, 0.30640669, 663), (0.82608696, 0.30640669, 664), (0.82789855, 0.30640669, 665), (0.82971014, 0.30640669, 666), (0.83152174, 0.30640669, 667), (0.83333333, 0.30362117, 668), (0.83514493, 0.30362117, 669), (0.83695652, 0.30362117, 670), (0.83876812, 0.30362117, 671), (0.84057971, 0.30362117, 672), (0.8423913, 0.30362117, 673), (0.8442029, 0.30362117, 674), (0.84601449, 0.30362117, 675), (0.84782609, 0.30362117, 676), (0.84963768, 0.30083565, 677), (0.85144928, 0.30083565, 678), (0.85326087, 0.30083565, 679), (0.85507246, 0.30083565, 680), (0.85688406, 0.30083565, 681), (0.85869565, 0.30083565, 682), (0.86050725, 0.30083565, 683), (0.86231884, 0.29805014, 684), (0.86413043, 0.29805014, 685), (0.86594203, 0.29805014, 686), (0.86775362, 0.29805014, 687), (0.86956522, 0.29805014, 688), (0.87137681, 0.29805014, 689), (0.87318841, 0.29526462, 690), (0.875, 0.29526462, 691), (0.87681159, 0.29526462, 692), (0.87862319, 0.29526462, 693), (0.88043478, 0.29526462, 694), (0.88224638, 0.29526462, 695)], [(0.49637681, 0.28690808, 696), (0.49818841, 0.28412256, 697), (0.5, 0.28412256, 698), (0.50181159, 0.28412256, 699), (0.50362319, 0.28412256, 700), (0.50543478, 0.28412256, 701), (0.50724638, 0.28412256, 702), (0.50905797, 0.28412256, 703), (0.51086957, 0.28412256, 704), (0.51268116, 0.28412256, 705), (0.51449275, 0.28412256, 706), (0.51630435, 0.28412256, 707), (0.51811594, 0.28412256, 708), (0.51992754, 0.28412256, 709), (0.52173913, 0.28412256, 710), (0.52355072, 0.28412256, 711), (0.52536232, 0.28412256, 712), (0.52717391, 0.28412256, 713), (0.52898551, 0.28412256, 714), (0.5307971, 0.28412256, 715), (0.5326087, 0.28412256, 716), (0.53442029, 0.28412256, 717), (0.53623188, 0.28412256, 718), (0.53804348, 0.28412256, 719), (0.53985507, 0.28412256, 720), (0.54166667, 0.28412256, 721), (0.54347826, 0.28412256, 722), (0.54528986, 0.28412256, 723), (0.54710145, 0.28412256, 724), (0.54891304, 0.28412256, 725), (0.55072464, 0.28412256, 726), (0.55253623, 0.28412256, 727), (0.55434783, 0.28412256, 728), (0.55615942, 0.28412256, 729), (0.55797101, 0.28412256, 730), (0.55978261, 0.28412256, 731), (0.5615942, 0.28412256, 732), (0.5634058, 0.28412256, 733), (0.56521739, 0.28412256, 734), (0.56702899, 0.28412256, 735), (0.56884058, 0.28412256, 736), (0.57065217, 0.28412256, 737), (0.57246377, 0.28412256, 738), (0.57427536, 0.28412256, 739), (0.57608696, 0.28412256, 740), (0.57789855, 0.28412256, 741), (0.57971014, 0.28412256, 742), (0.58152174, 0.28412256, 743), (0.58333333, 0.28412256, 744), (0.58514493, 0.28412256, 745), (0.58695652, 0.28412256, 746), (0.58876812, 0.28412256, 747), (0.59057971, 0.28412256, 748), (0.5923913, 0.28412256, 749), (0.5942029, 0.28412256, 750), (0.59601449, 0.28412256, 751), (0.59782609, 0.28412256, 752), (0.59963768, 0.28412256, 753), (0.60144928, 0.28690808, 754), (0.60144928, 0.28969359, 755), (0.59963768, 0.29247911, 756), (0.59782609, 0.29247911, 757), (0.59601449, 0.29247911, 758), (0.5942029, 0.29247911, 759), (0.5923913, 0.29247911, 760), (0.59057971, 0.29247911, 761), (0.58876812, 0.29247911, 762), (0.58695652, 0.29247911, 763), (0.58514493, 0.29247911, 764), (0.58333333, 0.29247911, 765), (0.58152174, 0.29247911, 766), (0.57971014, 0.29247911, 767), (0.57789855, 0.29247911, 768), (0.57608696, 0.29247911, 769), (0.57427536, 0.29247911, 770), (0.57246377, 0.29247911, 771), (0.57065217, 0.29247911, 772), (0.56884058, 0.29247911, 773), (0.56702899, 0.29247911, 774), (0.56521739, 0.29247911, 775), (0.5634058, 0.29247911, 776), (0.5615942, 0.29247911, 777), (0.55978261, 0.29247911, 778), (0.55797101, 0.29247911, 779), (0.55615942, 0.29247911, 780), (0.55434783, 0.29247911, 781), (0.55253623, 0.29247911, 782), (0.55072464, 0.29247911, 783), (0.54891304, 0.29247911, 784), (0.54710145, 0.29247911, 785), (0.54528986, 0.29247911, 786), (0.54347826, 0.29247911, 787), (0.54166667, 0.29247911, 788), (0.53985507, 0.29247911, 789), (0.53804348, 0.29247911, 790), (0.53623188, 0.29247911, 791), (0.53442029, 0.29247911, 792), (0.5326087, 0.29247911, 793), (0.5307971, 0.29247911, 794), (0.52898551, 0.29247911, 795), (0.52717391, 0.29247911, 796), (0.52536232, 0.29247911, 797), (0.52355072, 0.29247911, 798), (0.52173913, 0.29247911, 799), (0.51992754, 0.29247911, 800), (0.51811594, 0.29247911, 801), (0.51630435, 0.29247911, 802), (0.51449275, 0.29247911, 803), (0.51268116, 0.29247911, 804), (0.51086957, 0.29247911, 805), (0.50905797, 0.29247911, 806), (0.50724638, 0.29247911, 807), (0.50724638, 0.29526462, 808), (0.50905797, 0.29805014, 809), (0.50905797, 0.30083565, 810), (0.51086957, 0.30362117, 811), (0.51086957, 0.30640669, 812), (0.51086957, 0.3091922, 813), (0.51086957, 0.31197772, 814), (0.51086957, 0.31476323, 815), (0.51268116, 0.31754875, 816), (0.51268116, 0.32033426, 817), (0.51449275, 0.32311978, 818), (0.51449275, 0.32590529, 819), (0.51449275, 0.32869081, 820), (0.51630435, 0.33147632, 821), (0.51630435, 0.33426184, 822), (0.51811594, 0.33704735, 823), (0.51811594, 0.33983287, 824), (0.51992754, 0.34261838, 825), (0.51992754, 0.3454039, 826), (0.51992754, 0.34818942, 827), (0.52173913, 0.35097493, 828), (0.52173913, 0.35376045, 829), (0.52173913, 0.35654596, 830), (0.52355072, 0.35933148, 831), (0.52536232, 0.36211699, 832), (0.52536232, 0.36490251, 833), (0.52717391, 0.36768802, 834), (0.52898551, 0.37047354, 835), (0.52898551, 0.37325905, 836), (0.5307971, 0.37604457, 837), (0.5307971, 0.37883008, 838), (0.5307971, 0.3816156, 839), (0.5326087, 0.3816156, 840), (0.53442029, 0.38440111, 841), (0.53442029, 0.38718663, 842), (0.53623188, 0.38718663, 843), (0.53804348, 0.38718663, 844), (0.53985507, 0.38718663, 845), (0.54166667, 0.38718663, 846), (0.54347826, 0.38718663, 847), (0.54528986, 0.38718663, 848), (0.54710145, 0.38718663, 849), (0.54891304, 0.38718663, 850), (0.55072464, 0.38718663, 851), (0.55253623, 0.38718663, 852), (0.55434783, 0.38718663, 853), (0.55615942, 0.38440111, 854), (0.55797101, 0.38440111, 855), (0.55978261, 0.38440111, 856), (0.5615942, 0.38440111, 857), (0.5634058, 0.38440111, 858), (0.56521739, 0.3816156, 859), (0.56702899, 0.3816156, 860), (0.56884058, 0.3816156, 861), (0.57065217, 0.37883008, 862), (0.57246377, 0.37883008, 863), (0.57427536, 0.37883008, 864), (0.57608696, 0.37883008, 865), (0.57789855, 0.37883008, 866), (0.57971014, 0.37883008, 867), (0.58152174, 0.37883008, 868), (0.58333333, 0.37883008, 869), (0.58514493, 0.37883008, 870), (0.58695652, 0.37883008, 871), (0.58876812, 0.37883008, 872), (0.59057971, 0.3816156, 873), (0.5923913, 0.3816156, 874), (0.5942029, 0.3816156, 875), (0.59601449, 0.38440111, 876), (0.59601449, 0.38718663, 877), (0.59782609, 0.38718663, 878), (0.59963768, 0.38997214, 879), (0.59963768, 0.39275766, 880), (0.60144928, 0.39554318, 881), (0.60326087, 0.39832869, 882), (0.60507246, 0.40111421, 883), (0.60507246, 0.40389972, 884), (0.60688406, 0.40668524, 885), (0.60688406, 0.40947075, 886), (0.60869565, 0.41225627, 887), (0.61050725, 0.41504178, 888), (0.61050725, 0.4178273, 889), (0.61231884, 0.42061281, 890), (0.61231884, 0.42339833, 891), (0.61413043, 0.42618384, 892), (0.61413043, 0.42896936, 893), (0.61594203, 0.43175487, 894), (0.61775362, 0.43454039, 895), (0.61956522, 0.43732591, 896), (0.61956522, 0.44011142, 897), (0.61956522, 0.44289694, 898), (0.62137681, 0.44568245, 899), (0.62137681, 0.44846797, 900), (0.62137681, 0.45125348, 901), (0.62137681, 0.454039, 902), (0.62137681, 0.45682451, 903), (0.61956522, 0.45961003, 904), (0.61775362, 0.45961003, 905), (0.61775362, 0.46239554, 906), (0.61594203, 0.46518106, 907), (0.61413043, 0.46518106, 908), (0.61231884, 0.46796657, 909), (0.61050725, 0.46796657, 910), (0.60869565, 0.47075209, 911), (0.60688406, 0.47075209, 912), (0.60507246, 0.4735376, 913), (0.60326087, 0.47632312, 914), (0.60144928, 0.47910864, 915), (0.59963768, 0.48189415, 916), (0.59782609, 0.48189415, 917), (0.59601449, 0.48467967, 918), (0.5942029, 0.48746518, 919), (0.5923913, 0.48746518, 920), (0.59057971, 0.4902507, 921), (0.58876812, 0.49303621, 922), (0.58695652, 0.49303621, 923), (0.58514493, 0.49582173, 924), (0.58333333, 0.49860724, 925), (0.58152174, 0.50139276, 926), (0.57971014, 0.50139276, 927), (0.57789855, 0.50417827, 928), (0.57608696, 0.50696379, 929), (0.57427536, 0.5097493, 930), (0.57246377, 0.5097493, 931), (0.57065217, 0.5097493, 932), (0.56884058, 0.5097493, 933), (0.56884058, 0.51253482, 934), (0.56702899, 0.51532033, 935), (0.56521739, 0.51532033, 936), (0.5634058, 0.51532033, 937), (0.5615942, 0.51810585, 938), (0.55978261, 0.51810585, 939), (0.55797101, 0.52089136, 940), (0.55615942, 0.52089136, 941), (0.55434783, 0.52089136, 942), (0.55253623, 0.52089136, 943), (0.55072464, 0.52089136, 944), (0.54891304, 0.52367688, 945), (0.54710145, 0.52367688, 946), (0.54528986, 0.5264624, 947), (0.54347826, 0.5264624, 948), (0.54166667, 0.5264624, 949), (0.53985507, 0.5264624, 950), (0.53804348, 0.5264624, 951), (0.53623188, 0.5264624, 952), (0.53442029, 0.52367688, 953), (0.53442029, 0.52089136, 954), (0.53442029, 0.51810585, 955), (0.53623188, 0.51532033, 956), (0.53804348, 0.51532033, 957), (0.53985507, 0.51810585, 958), (0.54166667, 0.51810585, 959), (0.54347826, 0.51810585, 960), (0.54528986, 0.51532033, 961), (0.54710145, 0.51532033, 962), (0.54891304, 0.51253482, 963), (0.55072464, 0.51253482, 964), (0.55253623, 0.51253482, 965), (0.55434783, 0.51253482, 966), (0.55615942, 0.51253482, 967), (0.55797101, 0.51253482, 968), (0.55978261, 0.5097493, 969), (0.5615942, 0.50696379, 970), (0.5634058, 0.50696379, 971), (0.56521739, 0.50417827, 972), (0.56702899, 0.50417827, 973), (0.56884058, 0.50139276, 974), (0.57065217, 0.50139276, 975), (0.57246377, 0.50139276, 976), (0.57427536, 0.49860724, 977), (0.57608696, 0.49582173, 978), (0.57789855, 0.49582173, 979), (0.57971014, 0.49303621, 980), (0.58152174, 0.4902507, 981), (0.58333333, 0.4902507, 982), (0.58514493, 0.48746518, 983), (0.58695652, 0.48467967, 984), (0.58876812, 0.48189415, 985), (0.59057971, 0.48189415, 986), (0.5923913, 0.47910864, 987), (0.5942029, 0.47632312, 988), (0.59601449, 0.47632312, 989), (0.59782609, 0.4735376, 990), (0.59963768, 0.47075209, 991), (0.60144928, 0.46796657, 992), (0.60326087, 0.46518106, 993), (0.60507246, 0.46518106, 994), (0.60688406, 0.46239554, 995), (0.60869565, 0.46239554, 996), (0.61050725, 0.45961003, 997), (0.61231884, 0.45961003, 998), (0.61231884, 0.45682451, 999), (0.61413043, 0.454039, 1000), (0.61594203, 0.454039, 1001), (0.61594203, 0.45125348, 1002), (0.61594203, 0.44846797, 1003), (0.61413043, 0.44568245, 1004), (0.61413043, 0.44289694, 1005), (0.61413043, 0.44011142, 1006), (0.61231884, 0.43732591, 1007), (0.61050725, 0.43454039, 1008), (0.60869565, 0.43175487, 1009), (0.60869565, 0.42896936, 1010), (0.60688406, 0.42618384, 1011), (0.60688406, 0.42339833, 1012), (0.60507246, 0.42061281, 1013), (0.60507246, 0.4178273, 1014), (0.60326087, 0.41504178, 1015), (0.60326087, 0.41225627, 1016), (0.60144928, 0.40947075, 1017), (0.59963768, 0.40668524, 1018), (0.59963768, 0.40389972, 1019), (0.59782609, 0.40111421, 1020), (0.59601449, 0.39832869, 1021), (0.59601449, 0.39554318, 1022), (0.5942029, 0.39554318, 1023), (0.5923913, 0.39275766, 1024), (0.5923913, 0.38997214, 1025), (0.59057971, 0.38997214, 1026), (0.58876812, 0.38997214, 1027), (0.58695652, 0.38718663, 1028), (0.58514493, 0.38718663, 1029), (0.58333333, 0.38718663, 1030), (0.58152174, 0.38718663, 1031), (0.57971014, 0.38718663, 1032), (0.57789855, 0.38718663, 1033), (0.57608696, 0.38718663, 1034), (0.57427536, 0.38718663, 1035), (0.57246377, 0.38718663, 1036), (0.57065217, 0.38997214, 1037), (0.56884058, 0.38997214, 1038), (0.56702899, 0.38997214, 1039), (0.56521739, 0.38997214, 1040), (0.5634058, 0.39275766, 1041), (0.5615942, 0.39275766, 1042), (0.55978261, 0.39275766, 1043), (0.55797101, 0.39275766, 1044), (0.55615942, 0.39275766, 1045), (0.55434783, 0.39554318, 1046), (0.55253623, 0.39554318, 1047), (0.55072464, 0.39554318, 1048), (0.54891304, 0.39554318, 1049), (0.54710145, 0.39554318, 1050), (0.54528986, 0.39554318, 1051), (0.54347826, 0.39554318, 1052), (0.54166667, 0.39554318, 1053), (0.53985507, 0.39554318, 1054), (0.53804348, 0.39554318, 1055), (0.53623188, 0.39554318, 1056), (0.53442029, 0.39554318, 1057), (0.5326087, 0.39554318, 1058), (0.5307971, 0.39554318, 1059), (0.52898551, 0.39275766, 1060), (0.52898551, 0.38997214, 1061), (0.52717391, 0.38718663, 1062), (0.52536232, 0.38440111, 1063), (0.52536232, 0.3816156, 1064), (0.52355072, 0.37883008, 1065), (0.52355072, 0.37604457, 1066), (0.52355072, 0.37325905, 1067), (0.52173913, 0.37325905, 1068), (0.51992754, 0.37047354, 1069), (0.51992754, 0.36768802, 1070), (0.51992754, 0.36490251, 1071), (0.51811594, 0.36211699, 1072), (0.51811594, 0.35933148, 1073), (0.51630435, 0.35654596, 1074), (0.51630435, 0.35376045, 1075), (0.51449275, 0.35097493, 1076), (0.51449275, 0.34818942, 1077), (0.51449275, 0.3454039, 1078), (0.51268116, 0.34261838, 1079), (0.51268116, 0.33983287, 1080), (0.51268116, 0.33704735, 1081), (0.51086957, 0.33426184, 1082), (0.51086957, 0.33147632, 1083), (0.50905797, 0.32869081, 1084), (0.50905797, 0.32590529, 1085), (0.50724638, 0.32311978, 1086), (0.50724638, 0.32033426, 1087), (0.50724638, 0.31754875, 1088), (0.50543478, 0.31476323, 1089), (0.50543478, 0.31197772, 1090), (0.50543478, 0.3091922, 1091), (0.50543478, 0.30640669, 1092), (0.50543478, 0.30362117, 1093), (0.50362319, 0.30083565, 1094), (0.50181159, 0.29805014, 1095), (0.50181159, 0.29526462, 1096), (0.5, 0.29526462, 1097), (0.49818841, 0.29247911, 1098), (0.49637681, 0.28969359, 1099)], [(0.16304348, 0.26740947, 1100), (0.16485507, 0.26462396, 1101), (0.16666667, 0.26462396, 1102), (0.16847826, 0.26462396, 1103), (0.17028986, 0.26740947, 1104), (0.17028986, 0.27019499, 1105), (0.16847826, 0.2729805, 1106), (0.16666667, 0.2729805, 1107), (0.16485507, 0.27576602, 1108), (0.16304348, 0.27576602, 1109), (0.16123188, 0.27576602, 1110), (0.15942029, 0.27576602, 1111), (0.1576087, 0.27855153, 1112), (0.1557971, 0.27855153, 1113), (0.15398551, 0.27855153, 1114), (0.15217391, 0.28133705, 1115), (0.15036232, 0.28412256, 1116), (0.14855072, 0.28412256, 1117), (0.14673913, 0.28690808, 1118), (0.14492754, 0.28690808, 1119), (0.14311594, 0.28690808, 1120), (0.14130435, 0.28969359, 1121), (0.13949275, 0.28969359, 1122), (0.13768116, 0.28969359, 1123), (0.13586957, 0.29247911, 1124), (0.13405797, 0.29247911, 1125), (0.13224638, 0.29526462, 1126), (0.13043478, 0.29526462, 1127), (0.12862319, 0.29526462, 1128), (0.12681159, 0.29526462, 1129), (0.125, 0.29526462, 1130), (0.12318841, 0.29526462, 1131), (0.12137681, 0.29805014, 1132), (0.11956522, 0.29805014, 1133), (0.11775362, 0.29805014, 1134), (0.11594203, 0.29805014, 1135), (0.11413043, 0.30083565, 1136), (0.11231884, 0.30083565, 1137), (0.11050725, 0.30362117, 1138), (0.10869565, 0.30362117, 1139), (0.10688406, 0.30362117, 1140), (0.10507246, 0.30640669, 1141), (0.10326087, 0.30640669, 1142), (0.10144928, 0.30640669, 1143), (0.09963768, 0.30640669, 1144), (0.09782609, 0.30640669, 1145), (0.09601449, 0.30640669, 1146), (0.0942029, 0.30640669, 1147), (0.0923913, 0.3091922, 1148), (0.09057971, 0.3091922, 1149), (0.08876812, 0.3091922, 1150), (0.08695652, 0.3091922, 1151), (0.08514493, 0.3091922, 1152), (0.08333333, 0.3091922, 1153), (0.08333333, 0.31197772, 1154), (0.08514493, 0.31476323, 1155), (0.08514493, 0.31754875, 1156), (0.08514493, 0.32033426, 1157), (0.08514493, 0.32311978, 1158), (0.08695652, 0.32590529, 1159), (0.08695652, 0.32869081, 1160), (0.08876812, 0.33147632, 1161), (0.08876812, 0.33426184, 1162), (0.09057971, 0.33704735, 1163), (0.0923913, 0.33983287, 1164), (0.0923913, 0.34261838, 1165), (0.0942029, 0.3454039, 1166), (0.0942029, 0.34818942, 1167), (0.09601449, 0.35097493, 1168), (0.09601449, 0.35376045, 1169), (0.09782609, 0.35654596, 1170), (0.09782609, 0.35933148, 1171), (0.09782609, 0.36211699, 1172), (0.09963768, 0.36490251, 1173), (0.09963768, 0.36768802, 1174), (0.10144928, 0.37047354, 1175), (0.10144928, 0.37325905, 1176), (0.10326087, 0.37604457, 1177), (0.10326087, 0.37883008, 1178), (0.10326087, 0.3816156, 1179), (0.10507246, 0.38440111, 1180), (0.10507246, 0.38718663, 1181), (0.10688406, 0.38997214, 1182), (0.10688406, 0.39275766, 1183), (0.10869565, 0.39554318, 1184), (0.10869565, 0.39832869, 1185), (0.10869565, 0.40111421, 1186), (0.11050725, 0.40389972, 1187), (0.11050725, 0.40668524, 1188), (0.11231884, 0.40947075, 1189), (0.11231884, 0.41225627, 1190), (0.11231884, 0.41504178, 1191), (0.11413043, 0.4178273, 1192), (0.11413043, 0.42061281, 1193), (0.11413043, 0.42339833, 1194), (0.11594203, 0.42618384, 1195), (0.11594203, 0.42896936, 1196), (0.11594203, 0.43175487, 1197), (0.11775362, 0.43454039, 1198), (0.11775362, 0.43732591, 1199), (0.11956522, 0.44011142, 1200), (0.11956522, 0.44289694, 1201), (0.11956522, 0.44568245, 1202), (0.12137681, 0.44846797, 1203), (0.12318841, 0.45125348, 1204), (0.12318841, 0.454039, 1205), (0.12318841, 0.45682451, 1206), (0.125, 0.45961003, 1207), (0.12681159, 0.46239554, 1208), (0.12862319, 0.46518106, 1209), (0.13043478, 0.46518106, 1210), (0.13224638, 0.46518106, 1211), (0.13405797, 0.46518106, 1212), (0.13586957, 0.46518106, 1213), (0.13768116, 0.46518106, 1214), (0.13949275, 0.46518106, 1215), (0.14130435, 0.46518106, 1216), (0.14311594, 0.46518106, 1217), (0.14492754, 0.46518106, 1218), (0.14673913, 0.46518106, 1219), (0.14855072, 0.46518106, 1220), (0.15036232, 0.46518106, 1221), (0.15217391, 0.46239554, 1222), (0.15398551, 0.46239554, 1223), (0.1557971, 0.46239554, 1224), (0.1576087, 0.46239554, 1225), (0.15942029, 0.45961003, 1226), (0.16123188, 0.45961003, 1227), (0.16304348, 0.45682451, 1228), (0.16485507, 0.45682451, 1229), (0.16666667, 0.454039, 1230), (0.16847826, 0.454039, 1231), (0.17028986, 0.454039, 1232), (0.17210145, 0.454039, 1233), (0.17391304, 0.454039, 1234), (0.17572464, 0.454039, 1235), (0.17753623, 0.45125348, 1236), (0.17934783, 0.45125348, 1237), (0.18115942, 0.45125348, 1238), (0.18297101, 0.44846797, 1239), (0.18478261, 0.44846797, 1240), (0.1865942, 0.44846797, 1241), (0.1884058, 0.44568245, 1242), (0.19021739, 0.44568245, 1243), (0.19202899, 0.44289694, 1244), (0.19384058, 0.44289694, 1245), (0.19565217, 0.44289694, 1246), (0.19746377, 0.44289694, 1247), (0.19927536, 0.44289694, 1248), (0.20108696, 0.44011142, 1249), (0.20289855, 0.44011142, 1250), (0.20471014, 0.43732591, 1251), (0.20652174, 0.43732591, 1252), (0.20833333, 0.43732591, 1253), (0.21014493, 0.43732591, 1254), (0.21195652, 0.43732591, 1255), (0.21376812, 0.44011142, 1256), (0.21376812, 0.44289694, 1257), (0.21376812, 0.44568245, 1258), (0.21376812, 0.44846797, 1259), (0.21376812, 0.45125348, 1260), (0.21376812, 0.454039, 1261), (0.21376812, 0.45682451, 1262), (0.21376812, 0.45961003, 1263), (0.21376812, 0.46239554, 1264), (0.21376812, 0.46518106, 1265), (0.21376812, 0.46796657, 1266), (0.21376812, 0.47075209, 1267), (0.21195652, 0.4735376, 1268), (0.21195652, 0.47632312, 1269), (0.21014493, 0.47910864, 1270), (0.20833333, 0.48189415, 1271), (0.20833333, 0.48467967, 1272), (0.20652174, 0.48746518, 1273), (0.20471014, 0.48746518, 1274), (0.20289855, 0.4902507, 1275), (0.20108696, 0.4902507, 1276), (0.19927536, 0.49303621, 1277), (0.19746377, 0.49303621, 1278), (0.19565217, 0.49582173, 1279), (0.19384058, 0.49582173, 1280), (0.19202899, 0.49582173, 1281), (0.19021739, 0.49860724, 1282), (0.1884058, 0.49860724, 1283), (0.1865942, 0.50139276, 1284), (0.18478261, 0.50139276, 1285), (0.18297101, 0.50139276, 1286), (0.18115942, 0.50417827, 1287), (0.17934783, 0.50417827, 1288), (0.17753623, 0.50696379, 1289), (0.17572464, 0.50696379, 1290), (0.17391304, 0.5097493, 1291), (0.17210145, 0.5097493, 1292), (0.17028986, 0.51253482, 1293), (0.16847826, 0.51253482, 1294), (0.16666667, 0.51532033, 1295), (0.16485507, 0.51810585, 1296), (0.16304348, 0.51810585, 1297), (0.16123188, 0.52089136, 1298), (0.15942029, 0.52089136, 1299), (0.1576087, 0.52367688, 1300), (0.1557971, 0.52367688, 1301), (0.15398551, 0.52367688, 1302), (0.15217391, 0.5264624, 1303), (0.15036232, 0.5264624, 1304), (0.14855072, 0.52924791, 1305), (0.14673913, 0.52924791, 1306), (0.14492754, 0.53203343, 1307), (0.14311594, 0.53203343, 1308), (0.14130435, 0.53203343, 1309), (0.13949275, 0.53481894, 1310), (0.13768116, 0.53481894, 1311), (0.13586957, 0.53481894, 1312), (0.13405797, 0.53760446, 1313), (0.13224638, 0.54038997, 1314), (0.13043478, 0.54038997, 1315), (0.12862319, 0.54038997, 1316), (0.12681159, 0.53760446, 1317), (0.12681159, 0.53481894, 1318), (0.12862319, 0.53203343, 1319), (0.13043478, 0.52924791, 1320), (0.13224638, 0.5264624, 1321), (0.13405797, 0.5264624, 1322), (0.13586957, 0.5264624, 1323), (0.13768116, 0.5264624, 1324), (0.13949275, 0.52367688, 1325), (0.14130435, 0.52367688, 1326), (0.14311594, 0.52367688, 1327), (0.14492754, 0.52367688, 1328), (0.14673913, 0.52089136, 1329), (0.14855072, 0.52089136, 1330), (0.15036232, 0.51810585, 1331), (0.15217391, 0.51810585, 1332), (0.15398551, 0.51532033, 1333), (0.1557971, 0.51532033, 1334), (0.1576087, 0.51253482, 1335), (0.15942029, 0.51253482, 1336), (0.16123188, 0.51253482, 1337), (0.16304348, 0.5097493, 1338), (0.16485507, 0.50696379, 1339), (0.16666667, 0.50696379, 1340), (0.16847826, 0.50417827, 1341), (0.17028986, 0.50417827, 1342), (0.17210145, 0.50139276, 1343), (0.17391304, 0.50139276, 1344), (0.17572464, 0.49860724, 1345), (0.17753623, 0.49860724, 1346), (0.17934783, 0.49582173, 1347), (0.18115942, 0.49582173, 1348), (0.18297101, 0.49303621, 1349), (0.18478261, 0.49303621, 1350), (0.1865942, 0.49303621, 1351), (0.1884058, 0.4902507, 1352), (0.19021739, 0.4902507, 1353), (0.19202899, 0.48746518, 1354), (0.19384058, 0.48746518, 1355), (0.19565217, 0.48746518, 1356), (0.19746377, 0.48467967, 1357), (0.19927536, 0.48467967, 1358), (0.20108696, 0.48189415, 1359), (0.20289855, 0.47910864, 1360), (0.20289855, 0.47632312, 1361), (0.20471014, 0.4735376, 1362), (0.20652174, 0.47075209, 1363), (0.20652174, 0.46796657, 1364), (0.20833333, 0.46518106, 1365), (0.20833333, 0.46239554, 1366), (0.20833333, 0.45961003, 1367), (0.20833333, 0.45682451, 1368), (0.20833333, 0.454039, 1369), (0.20833333, 0.45125348, 1370), (0.20833333, 0.44846797, 1371), (0.20833333, 0.44568245, 1372), (0.20652174, 0.44568245, 1373), (0.20471014, 0.44846797, 1374), (0.20289855, 0.44846797, 1375), (0.20108696, 0.45125348, 1376), (0.19927536, 0.45125348, 1377), (0.19746377, 0.45125348, 1378), (0.19565217, 0.45125348, 1379), (0.19384058, 0.45125348, 1380), (0.19202899, 0.45125348, 1381), (0.19021739, 0.454039, 1382), (0.1884058, 0.454039, 1383), (0.1865942, 0.45682451, 1384), (0.18478261, 0.45682451, 1385), (0.18297101, 0.45961003, 1386), (0.18115942, 0.45961003, 1387), (0.17934783, 0.45961003, 1388), (0.17753623, 0.46239554, 1389), (0.17572464, 0.46239554, 1390), (0.17391304, 0.46239554, 1391), (0.17210145, 0.46239554, 1392), (0.17028986, 0.46239554, 1393), (0.16847826, 0.46239554, 1394), (0.16666667, 0.46518106, 1395), (0.16485507, 0.46518106, 1396), (0.16304348, 0.46796657, 1397), (0.16123188, 0.46796657, 1398), (0.15942029, 0.46796657, 1399), (0.1576087, 0.47075209, 1400), (0.1557971, 0.47075209, 1401), (0.15398551, 0.47075209, 1402), (0.15217391, 0.47075209, 1403), (0.15036232, 0.4735376, 1404), (0.14855072, 0.4735376, 1405), (0.14673913, 0.4735376, 1406), (0.14492754, 0.4735376, 1407), (0.14311594, 0.4735376, 1408), (0.14130435, 0.4735376, 1409), (0.13949275, 0.4735376, 1410), (0.13768116, 0.4735376, 1411), (0.13586957, 0.4735376, 1412), (0.13405797, 0.4735376, 1413), (0.13224638, 0.4735376, 1414), (0.13043478, 0.4735376, 1415), (0.12862319, 0.4735376, 1416), (0.12681159, 0.4735376, 1417), (0.125, 0.4735376, 1418), (0.12318841, 0.47075209, 1419), (0.12137681, 0.46796657, 1420), (0.11956522, 0.46518106, 1421), (0.11775362, 0.46239554, 1422), (0.11775362, 0.45961003, 1423), (0.11775362, 0.45682451, 1424), (0.11775362, 0.454039, 1425), (0.11594203, 0.45125348, 1426), (0.11594203, 0.44846797, 1427), (0.11413043, 0.44568245, 1428), (0.11413043, 0.44289694, 1429), (0.11231884, 0.44011142, 1430), (0.11231884, 0.43732591, 1431), (0.11231884, 0.43454039, 1432), (0.11050725, 0.43175487, 1433), (0.11050725, 0.42896936, 1434), (0.11050725, 0.42618384, 1435), (0.10869565, 0.42339833, 1436), (0.10869565, 0.42061281, 1437), (0.10688406, 0.4178273, 1438), (0.10688406, 0.41504178, 1439), (0.10688406, 0.41225627, 1440), (0.10507246, 0.40947075, 1441), (0.10507246, 0.40668524, 1442), (0.10507246, 0.40389972, 1443), (0.10326087, 0.40111421, 1444), (0.10326087, 0.39832869, 1445), (0.10144928, 0.39554318, 1446), (0.10144928, 0.39275766, 1447), (0.10144928, 0.38997214, 1448), (0.09963768, 0.38718663, 1449), (0.09963768, 0.38440111, 1450), (0.09782609, 0.3816156, 1451), (0.09782609, 0.37883008, 1452), (0.09601449, 0.37604457, 1453), (0.09601449, 0.37325905, 1454), (0.09601449, 0.37047354, 1455), (0.0942029, 0.36768802, 1456), (0.0942029, 0.36490251, 1457), (0.0923913, 0.36211699, 1458), (0.0923913, 0.35933148, 1459), (0.09057971, 0.35654596, 1460), (0.09057971, 0.35376045, 1461), (0.09057971, 0.35097493, 1462), (0.08876812, 0.34818942, 1463), (0.08876812, 0.3454039, 1464), (0.08695652, 0.34261838, 1465), (0.08514493, 0.33983287, 1466), (0.08514493, 0.33704735, 1467), (0.08333333, 0.33426184, 1468), (0.08333333, 0.33147632, 1469), (0.08152174, 0.32869081, 1470), (0.08152174, 0.32590529, 1471), (0.07971014, 0.32311978, 1472), (0.07971014, 0.32033426, 1473), (0.07971014, 0.31754875, 1474), (0.07789855, 0.31476323, 1475), (0.07608696, 0.31197772, 1476), (0.07608696, 0.3091922, 1477), (0.07608696, 0.30640669, 1478), (0.07608696, 0.30362117, 1479), (0.07789855, 0.30083565, 1480), (0.07971014, 0.30083565, 1481), (0.08152174, 0.30083565, 1482), (0.08333333, 0.30083565, 1483), (0.08514493, 0.30083565, 1484), (0.08695652, 0.30083565, 1485), (0.08876812, 0.30083565, 1486), (0.09057971, 0.30083565, 1487), (0.0923913, 0.29805014, 1488), (0.0942029, 0.29805014, 1489), (0.09601449, 0.29805014, 1490), (0.09782609, 0.29805014, 1491), (0.09963768, 0.29805014, 1492), (0.10144928, 0.29805014, 1493), (0.10326087, 0.29805014, 1494), (0.10507246, 0.29526462, 1495), (0.10688406, 0.29526462, 1496), (0.10869565, 0.29526462, 1497), (0.11050725, 0.29526462, 1498), (0.11231884, 0.29247911, 1499), (0.11413043, 0.29247911, 1500), (0.11594203, 0.28969359, 1501), (0.11775362, 0.28969359, 1502), (0.11956522, 0.28969359, 1503), (0.12137681, 0.28690808, 1504), (0.12318841, 0.28690808, 1505), (0.125, 0.28690808, 1506), (0.12681159, 0.28690808, 1507), (0.12862319, 0.28690808, 1508), (0.13043478, 0.28690808, 1509), (0.13224638, 0.28690808, 1510), (0.13405797, 0.28412256, 1511), (0.13586957, 0.28412256, 1512), (0.13768116, 0.28133705, 1513), (0.13949275, 0.28133705, 1514), (0.14130435, 0.28133705, 1515), (0.14311594, 0.27855153, 1516), (0.14492754, 0.27855153, 1517), (0.14673913, 0.27855153, 1518), (0.14855072, 0.27576602, 1519), (0.15036232, 0.2729805, 1520), (0.15217391, 0.2729805, 1521), (0.15398551, 0.27019499, 1522), (0.1557971, 0.27019499, 1523), (0.1576087, 0.27019499, 1524), (0.15942029, 0.26740947, 1525), (0.16123188, 0.26740947, 1526)], [(0.0, 0.0, 1527), (0.0, 0.00278552, 1528), (0.0, 0.00557103, 1529), (0.0, 0.00835655, 1530), (0.0, 0.01114206, 1531), (0.0, 0.01392758, 1532), (0.0, 0.01671309, 1533), (0.0, 0.01949861, 1534), (0.0, 0.02228412, 1535), (0.0, 0.02506964, 1536), (0.0, 0.02785515, 1537), (0.0, 0.03064067, 1538), (0.0, 0.03342618, 1539), (0.0, 0.0362117, 1540), (0.0, 0.03899721, 1541), (0.0, 0.04178273, 1542), (0.0, 0.04456825, 1543), (0.0, 0.04735376, 1544), (0.0, 0.05013928, 1545), (0.0, 0.05292479, 1546), (0.0, 0.05571031, 1547), (0.0, 0.05849582, 1548), (0.0, 0.06128134, 1549), (0.0, 0.06406685, 1550), (0.0, 0.06685237, 1551), (0.0, 0.06963788, 1552), (0.0, 0.0724234, 1553), (0.0, 0.07520891, 1554), (0.0, 0.07799443, 1555), (0.0, 0.08077994, 1556), (0.0, 0.08356546, 1557), (0.0, 0.08635097, 1558), (0.0, 0.08913649, 1559), (0.0, 0.09192201, 1560), (0.0, 0.09470752, 1561), (0.0, 0.09749304, 1562), (0.0, 0.10027855, 1563), (0.0, 0.10306407, 1564), (0.0, 0.10584958, 1565), (0.0, 0.1086351, 1566), (0.0, 0.11142061, 1567), (0.0, 0.11420613, 1568), (0.0, 0.11699164, 1569), (0.0, 0.11977716, 1570), (0.0, 0.12256267, 1571), (0.0, 0.12534819, 1572), (0.0, 0.1281337, 1573), (0.0, 0.13091922, 1574), (0.0, 0.13370474, 1575), (0.0, 0.13649025, 1576), (0.0, 0.13927577, 1577), (0.0, 0.14206128, 1578), (0.0, 0.1448468, 1579), (0.0, 0.14763231, 1580), (0.0, 0.15041783, 1581), (0.0, 0.15320334, 1582), (0.0, 0.15598886, 1583), (0.0, 0.15877437, 1584), (0.0, 0.16155989, 1585), (0.0, 0.1643454, 1586), (0.0, 0.16713092, 1587), (0.0, 0.16991643, 1588), (0.0, 0.17270195, 1589), (0.0, 0.17548747, 1590), (0.0, 0.17827298, 1591), (0.0, 0.1810585, 1592), (0.0, 0.18384401, 1593), (0.0, 0.18662953, 1594), (0.0, 0.18941504, 1595), (0.0, 0.19220056, 1596), (0.0, 0.19498607, 1597), (0.0, 0.19777159, 1598), (0.0, 0.2005571, 1599), (0.0, 0.20334262, 1600), (0.0, 0.20612813, 1601), (0.0, 0.20891365, 1602), (0.0, 0.21169916, 1603), (0.0, 0.21448468, 1604), (0.0, 0.21727019, 1605), (0.0, 0.22005571, 1606), (0.0, 0.22284123, 1607), (0.0, 0.22562674, 1608), (0.0, 0.22841226, 1609), (0.0, 0.23119777, 1610), (0.0, 0.23398329, 1611), (0.0, 0.2367688, 1612), (0.0, 0.23955432, 1613), (0.0, 0.24233983, 1614), (0.0, 0.24512535, 1615), (0.0, 0.24791086, 1616), (0.0, 0.25069638, 1617), (0.0, 0.25348189, 1618), (0.0, 0.25626741, 1619), (0.0, 0.25905292, 1620), (0.0, 0.26183844, 1621), (0.0, 0.26462396, 1622), (0.0, 0.26740947, 1623), (0.0, 0.27019499, 1624), (0.0, 0.2729805, 1625), (0.0, 0.27576602, 1626), (0.0, 0.27855153, 1627), (0.0, 0.28133705, 1628), (0.0, 0.28412256, 1629), (0.0, 0.28690808, 1630), (0.0, 0.28969359, 1631), (0.0, 0.29247911, 1632), (0.0, 0.29526462, 1633), (0.0, 0.29805014, 1634), (0.0, 0.30083565, 1635), (0.0, 0.30362117, 1636), (0.0, 0.30640669, 1637), (0.0, 0.3091922, 1638), (0.0, 0.31197772, 1639), (0.0, 0.31476323, 1640), (0.0, 0.31754875, 1641), (0.0, 0.32033426, 1642), (0.0, 0.32311978, 1643), (0.0, 0.32590529, 1644), (0.0, 0.32869081, 1645), (0.0, 0.33147632, 1646), (0.0, 0.33426184, 1647), (0.0, 0.33704735, 1648), (0.0, 0.33983287, 1649), (0.0, 0.34261838, 1650), (0.0, 0.3454039, 1651), (0.0, 0.34818942, 1652), (0.0, 0.35097493, 1653), (0.0, 0.35376045, 1654), (0.0, 0.35654596, 1655), (0.0, 0.35933148, 1656), (0.0, 0.36211699, 1657), (0.0, 0.36490251, 1658), (0.0, 0.36768802, 1659), (0.0, 0.37047354, 1660), (0.0, 0.37325905, 1661), (0.0, 0.37604457, 1662), (0.0, 0.37883008, 1663), (0.0, 0.3816156, 1664), (0.0, 0.38440111, 1665), (0.0, 0.38718663, 1666), (0.0, 0.38997214, 1667), (0.0, 0.39275766, 1668), (0.0, 0.39554318, 1669), (0.0, 0.39832869, 1670), (0.0, 0.40111421, 1671), (0.0, 0.40389972, 1672), (0.0, 0.40668524, 1673), (0.0, 0.40947075, 1674), (0.0, 0.41225627, 1675), (0.0, 0.41504178, 1676), (0.0, 0.4178273, 1677), (0.0, 0.42061281, 1678), (0.0, 0.42339833, 1679), (0.0, 0.42618384, 1680), (0.0, 0.42896936, 1681), (0.0, 0.43175487, 1682), (0.0, 0.43454039, 1683), (0.0, 0.43732591, 1684), (0.0, 0.44011142, 1685), (0.0, 0.44289694, 1686), (0.0, 0.44568245, 1687), (0.0, 0.44846797, 1688), (0.0, 0.45125348, 1689), (0.0, 0.454039, 1690), (0.0, 0.45682451, 1691), (0.0, 0.45961003, 1692), (0.0, 0.46239554, 1693), (0.0, 0.46518106, 1694), (0.0, 0.46796657, 1695), (0.0, 0.47075209, 1696), (0.0, 0.4735376, 1697), (0.0, 0.47632312, 1698), (0.0, 0.47910864, 1699), (0.0, 0.48189415, 1700), (0.0, 0.48467967, 1701), (0.0, 0.48746518, 1702), (0.0, 0.4902507, 1703), (0.0, 0.49303621, 1704), (0.0, 0.49582173, 1705), (0.0, 0.49860724, 1706), (0.0, 0.50139276, 1707), (0.0, 0.50417827, 1708), (0.0, 0.50696379, 1709), (0.0, 0.5097493, 1710), (0.0, 0.51253482, 1711), (0.0, 0.51532033, 1712), (0.0, 0.51810585, 1713), (0.0, 0.52089136, 1714), (0.0, 0.52367688, 1715), (0.0, 0.5264624, 1716), (0.0, 0.52924791, 1717), (0.0, 0.53203343, 1718), (0.0, 0.53481894, 1719), (0.0, 0.53760446, 1720), (0.0, 0.54038997, 1721), (0.0, 0.54317549, 1722), (0.0, 0.545961, 1723), (0.0, 0.54874652, 1724), (0.0, 0.55153203, 1725), (0.0, 0.55431755, 1726), (0.0, 0.55710306, 1727), (0.0, 0.55988858, 1728), (0.0, 0.56267409, 1729), (0.0, 0.56545961, 1730), (0.0, 0.56824513, 1731), (0.0, 0.57103064, 1732), (0.0, 0.57381616, 1733), (0.0, 0.57660167, 1734), (0.0, 0.57938719, 1735), (0.0, 0.5821727, 1736), (0.0, 0.58495822, 1737), (0.0, 0.58774373, 1738), (0.0, 0.59052925, 1739), (0.0, 0.59331476, 1740), (0.0, 0.59610028, 1741), (0.0, 0.59888579, 1742), (0.0, 0.60167131, 1743), (0.0, 0.60445682, 1744), (0.0, 0.60724234, 1745), (0.0, 0.61002786, 1746), (0.0, 0.61281337, 1747), (0.0, 0.61559889, 1748), (0.0, 0.6183844, 1749), (0.0, 0.62116992, 1750), (0.0, 0.62395543, 1751), (0.0, 0.62674095, 1752), (0.0, 0.62952646, 1753), (0.0, 0.63231198, 1754), (0.0, 0.63509749, 1755), (0.0, 0.63788301, 1756), (0.0, 0.64066852, 1757), (0.0, 0.64345404, 1758), (0.0, 0.64623955, 1759), (0.0, 0.64902507, 1760), (0.0, 0.65181058, 1761), (0.0, 0.6545961, 1762), (0.0, 0.65738162, 1763), (0.0, 0.66016713, 1764), (0.0, 0.66295265, 1765), (0.0, 0.66573816, 1766), (0.0, 0.66852368, 1767), (0.0, 0.67130919, 1768), (0.0, 0.67409471, 1769), (0.0, 0.67688022, 1770), (0.0, 0.67966574, 1771), (0.0, 0.68245125, 1772), (0.0, 0.68523677, 1773), (0.0, 0.68802228, 1774), (0.0, 0.6908078, 1775), (0.0, 0.69359331, 1776), (0.0, 0.69637883, 1777), (0.0, 0.69916435, 1778), (0.0, 0.70194986, 1779), (0.0, 0.70473538, 1780), (0.0, 0.70752089, 1781), (0.0, 0.71030641, 1782), (0.0, 0.71309192, 1783), (0.0, 0.71587744, 1784), (0.0, 0.71866295, 1785), (0.0, 0.72144847, 1786), (0.0, 0.72423398, 1787), (0.0, 0.7270195, 1788), (0.0, 0.72980501, 1789), (0.0, 0.73259053, 1790), (0.0, 0.73537604, 1791), (0.0, 0.73816156, 1792), (0.0, 0.74094708, 1793), (0.0, 0.74373259, 1794), (0.0, 0.74651811, 1795), (0.0, 0.74930362, 1796), (0.0, 0.75208914, 1797), (0.0, 0.75487465, 1798), (0.0, 0.75766017, 1799), (0.0, 0.76044568, 1800), (0.0, 0.7632312, 1801), (0.0, 0.76601671, 1802), (0.0, 0.76880223, 1803), (0.0, 0.77158774, 1804), (0.0, 0.77437326, 1805), (0.0, 0.77715877, 1806), (0.0, 0.77994429, 1807), (0.0, 0.78272981, 1808), (0.0, 0.78551532, 1809), (0.0, 0.78830084, 1810), (0.0, 0.79108635, 1811), (0.0, 0.79387187, 1812), (0.0, 0.79665738, 1813), (0.0, 0.7994429, 1814), (0.0, 0.80222841, 1815), (0.0, 0.80501393, 1816), (0.0, 0.80779944, 1817), (0.0, 0.81058496, 1818), (0.0, 0.81337047, 1819), (0.0, 0.81615599, 1820), (0.0, 0.8189415, 1821), (0.0, 0.82172702, 1822), (0.0, 0.82451253, 1823), (0.0, 0.82729805, 1824), (0.0, 0.83008357, 1825), (0.0, 0.83286908, 1826), (0.0, 0.8356546, 1827), (0.0, 0.83844011, 1828), (0.0, 0.84122563, 1829), (0.0, 0.84401114, 1830), (0.0, 0.84679666, 1831), (0.0, 0.84958217, 1832), (0.0, 0.85236769, 1833), (0.0, 0.8551532, 1834), (0.0, 0.85793872, 1835), (0.0, 0.86072423, 1836), (0.0, 0.86350975, 1837), (0.0, 0.86629526, 1838), (0.0, 0.86908078, 1839), (0.0, 0.8718663, 1840), (0.0, 0.87465181, 1841), (0.0, 0.87743733, 1842), (0.0, 0.88022284, 1843), (0.0, 0.88300836, 1844), (0.0, 0.88579387, 1845), (0.0, 0.88857939, 1846), (0.0, 0.8913649, 1847), (0.0, 0.89415042, 1848), (0.0, 0.89693593, 1849), (0.0, 0.89972145, 1850), (0.0, 0.90250696, 1851), (0.0, 0.90529248, 1852), (0.0, 0.90807799, 1853), (0.0, 0.91086351, 1854), (0.0, 0.91364903, 1855), (0.0, 0.91643454, 1856), (0.0, 0.91922006, 1857), (0.0, 0.92200557, 1858), (0.0, 0.92479109, 1859), (0.0, 0.9275766, 1860), (0.0, 0.93036212, 1861), (0.0, 0.93314763, 1862), (0.0, 0.93593315, 1863), (0.0, 0.93871866, 1864), (0.0, 0.94150418, 1865), (0.0, 0.94428969, 1866), (0.0, 0.94707521, 1867), (0.0, 0.94986072, 1868), (0.0, 0.95264624, 1869), (0.0, 0.95543175, 1870), (0.0, 0.95821727, 1871), (0.0, 0.96100279, 1872), (0.0, 0.9637883, 1873), (0.0, 0.96657382, 1874), (0.0, 0.96935933, 1875), (0.0, 0.97214485, 1876), (0.0, 0.97493036, 1877), (0.0, 0.97771588, 1878), (0.0, 0.98050139, 1879), (0.0, 0.98328691, 1880), (0.0, 0.98607242, 1881), (0.0, 0.98885794, 1882), (0.0, 0.99164345, 1883), (0.0, 0.99442897, 1884), (0.0, 0.99721448, 1885), (0.00181159, 0.99721448, 1886), (0.00362319, 0.99721448, 1887), (0.00543478, 0.99721448, 1888), (0.00724638, 0.99721448, 1889), (0.00905797, 0.99721448, 1890), (0.01086957, 0.99721448, 1891), (0.01268116, 0.99721448, 1892), (0.01449275, 0.99721448, 1893), (0.01630435, 0.99721448, 1894), (0.01811594, 0.99721448, 1895), (0.01992754, 0.99721448, 1896), (0.02173913, 0.99721448, 1897), (0.02355072, 0.99721448, 1898), (0.02536232, 0.99721448, 1899), (0.02717391, 0.99721448, 1900), (0.02898551, 0.99721448, 1901), (0.0307971, 0.99721448, 1902), (0.0326087, 0.99721448, 1903), (0.03442029, 0.99721448, 1904), (0.03623188, 0.99721448, 1905), (0.03804348, 0.99721448, 1906), (0.03985507, 0.99721448, 1907), (0.04166667, 0.99721448, 1908), (0.04347826, 0.99721448, 1909), (0.04528986, 0.99721448, 1910), (0.04710145, 0.99721448, 1911), (0.04891304, 0.99721448, 1912), (0.05072464, 0.99721448, 1913), (0.05253623, 0.99721448, 1914), (0.05434783, 0.99721448, 1915), (0.05615942, 0.99721448, 1916), (0.05797101, 0.99721448, 1917), (0.05978261, 0.99721448, 1918), (0.0615942, 0.99721448, 1919), (0.0634058, 0.99721448, 1920), (0.06521739, 0.99721448, 1921), (0.06702899, 0.99721448, 1922), (0.06884058, 0.99721448, 1923), (0.07065217, 0.99721448, 1924), (0.07246377, 0.99721448, 1925), (0.07427536, 0.99721448, 1926), (0.07608696, 0.99721448, 1927), (0.07789855, 0.99721448, 1928), (0.07971014, 0.99721448, 1929), (0.08152174, 0.99721448, 1930), (0.08333333, 0.99721448, 1931), (0.08514493, 0.99721448, 1932), (0.08695652, 0.99721448, 1933), (0.08876812, 0.99721448, 1934), (0.09057971, 0.99721448, 1935), (0.0923913, 0.99721448, 1936), (0.0942029, 0.99721448, 1937), (0.09601449, 0.99721448, 1938), (0.09782609, 0.99721448, 1939), (0.09963768, 0.99721448, 1940), (0.10144928, 0.99721448, 1941), (0.10326087, 0.99721448, 1942), (0.10507246, 0.99721448, 1943), (0.10688406, 0.99721448, 1944), (0.10869565, 0.99721448, 1945), (0.11050725, 0.99721448, 1946), (0.11231884, 0.99721448, 1947), (0.11413043, 0.99721448, 1948), (0.11594203, 0.99721448, 1949), (0.11775362, 0.99721448, 1950), (0.11956522, 0.99721448, 1951), (0.12137681, 0.99721448, 1952), (0.12318841, 0.99721448, 1953), (0.125, 0.99721448, 1954), (0.12681159, 0.99721448, 1955), (0.12862319, 0.99721448, 1956), (0.13043478, 0.99721448, 1957), (0.13224638, 0.99721448, 1958), (0.13405797, 0.99721448, 1959), (0.13586957, 0.99721448, 1960), (0.13768116, 0.99721448, 1961), (0.13949275, 0.99721448, 1962), (0.14130435, 0.99721448, 1963), (0.14311594, 0.99721448, 1964), (0.14492754, 0.99721448, 1965), (0.14673913, 0.99721448, 1966), (0.14855072, 0.99721448, 1967), (0.15036232, 0.99721448, 1968), (0.15217391, 0.99721448, 1969), (0.15398551, 0.99721448, 1970), (0.1557971, 0.99721448, 1971), (0.1576087, 0.99721448, 1972), (0.15942029, 0.99721448, 1973), (0.16123188, 0.99721448, 1974), (0.16304348, 0.99721448, 1975), (0.16485507, 0.99721448, 1976), (0.16666667, 0.99721448, 1977), (0.16847826, 0.99721448, 1978), (0.17028986, 0.99721448, 1979), (0.17210145, 0.99721448, 1980), (0.17391304, 0.99721448, 1981), (0.17572464, 0.99721448, 1982), (0.17753623, 0.99721448, 1983), (0.17934783, 0.99721448, 1984), (0.18115942, 0.99721448, 1985), (0.18297101, 0.99721448, 1986), (0.18478261, 0.99721448, 1987), (0.1865942, 0.99721448, 1988), (0.1884058, 0.99721448, 1989), (0.19021739, 0.99721448, 1990), (0.19202899, 0.99721448, 1991), (0.19384058, 0.99721448, 1992), (0.19565217, 0.99721448, 1993), (0.19746377, 0.99721448, 1994), (0.19927536, 0.99721448, 1995), (0.20108696, 0.99721448, 1996), (0.20289855, 0.99721448, 1997), (0.20471014, 0.99721448, 1998), (0.20652174, 0.99721448, 1999), (0.20833333, 0.99721448, 2000), (0.21014493, 0.99721448, 2001), (0.21195652, 0.99721448, 2002), (0.21376812, 0.99721448, 2003), (0.21557971, 0.99721448, 2004), (0.2173913, 0.99721448, 2005), (0.2192029, 0.99721448, 2006), (0.22101449, 0.99721448, 2007), (0.22282609, 0.99721448, 2008), (0.22463768, 0.99721448, 2009), (0.22644928, 0.99721448, 2010), (0.22826087, 0.99721448, 2011), (0.23007246, 0.99721448, 2012), (0.23188406, 0.99721448, 2013), (0.23369565, 0.99721448, 2014), (0.23550725, 0.99721448, 2015), (0.23731884, 0.99721448, 2016), (0.23913043, 0.99721448, 2017), (0.24094203, 0.99721448, 2018), (0.24275362, 0.99721448, 2019), (0.24456522, 0.99721448, 2020), (0.24637681, 0.99721448, 2021), (0.24818841, 0.99721448, 2022), (0.25, 0.99721448, 2023), (0.25181159, 0.99721448, 2024), (0.25362319, 0.99721448, 2025), (0.25543478, 0.99721448, 2026), (0.25724638, 0.99721448, 2027), (0.25905797, 0.99721448, 2028), (0.26086957, 0.99721448, 2029), (0.26268116, 0.99721448, 2030), (0.26449275, 0.99721448, 2031), (0.26630435, 0.99721448, 2032), (0.26811594, 0.99721448, 2033), (0.26992754, 0.99721448, 2034), (0.27173913, 0.99721448, 2035), (0.27355072, 0.99721448, 2036), (0.27536232, 0.99721448, 2037), (0.27717391, 0.99721448, 2038), (0.27898551, 0.99721448, 2039), (0.2807971, 0.99721448, 2040), (0.2826087, 0.99721448, 2041), (0.28442029, 0.99721448, 2042), (0.28623188, 0.99721448, 2043), (0.28804348, 0.99721448, 2044), (0.28985507, 0.99721448, 2045), (0.29166667, 0.99721448, 2046), (0.29347826, 0.99721448, 2047), (0.29528986, 0.99721448, 2048), (0.29710145, 0.99721448, 2049), (0.29891304, 0.99721448, 2050), (0.30072464, 0.99721448, 2051), (0.30253623, 0.99721448, 2052), (0.30434783, 0.99721448, 2053), (0.30615942, 0.99721448, 2054), (0.30797101, 0.99721448, 2055), (0.30978261, 0.99721448, 2056), (0.3115942, 0.99721448, 2057), (0.3134058, 0.99721448, 2058), (0.31521739, 0.99721448, 2059), (0.31702899, 0.99721448, 2060), (0.31884058, 0.99721448, 2061), (0.32065217, 0.99721448, 2062), (0.32246377, 0.99721448, 2063), (0.32427536, 0.99721448, 2064), (0.32608696, 0.99721448, 2065), (0.32789855, 0.99721448, 2066), (0.32971014, 0.99721448, 2067), (0.33152174, 0.99721448, 2068), (0.33333333, 0.99721448, 2069), (0.33514493, 0.99721448, 2070), (0.33695652, 0.99721448, 2071), (0.33876812, 0.99721448, 2072), (0.34057971, 0.99721448, 2073), (0.3423913, 0.99721448, 2074), (0.3442029, 0.99721448, 2075), (0.34601449, 0.99721448, 2076), (0.34782609, 0.99721448, 2077), (0.34963768, 0.99721448, 2078), (0.35144928, 0.99721448, 2079), (0.35326087, 0.99721448, 2080), (0.35507246, 0.99721448, 2081), (0.35688406, 0.99721448, 2082), (0.35869565, 0.99721448, 2083), (0.36050725, 0.99721448, 2084), (0.36231884, 0.99721448, 2085), (0.36413043, 0.99721448, 2086), (0.36594203, 0.99721448, 2087), (0.36775362, 0.99721448, 2088), (0.36956522, 0.99721448, 2089), (0.37137681, 0.99721448, 2090), (0.37318841, 0.99721448, 2091), (0.375, 0.99721448, 2092), (0.37681159, 0.99721448, 2093), (0.37862319, 0.99721448, 2094), (0.38043478, 0.99721448, 2095), (0.38224638, 0.99721448, 2096), (0.38405797, 0.99721448, 2097), (0.38586957, 0.99721448, 2098), (0.38768116, 0.99721448, 2099), (0.38949275, 0.99721448, 2100), (0.39130435, 0.99721448, 2101), (0.39311594, 0.99721448, 2102), (0.39492754, 0.99721448, 2103), (0.39673913, 0.99721448, 2104), (0.39855072, 0.99721448, 2105), (0.40036232, 0.99721448, 2106), (0.40217391, 0.99721448, 2107), (0.40398551, 0.99721448, 2108), (0.4057971, 0.99721448, 2109), (0.4076087, 0.99721448, 2110), (0.40942029, 0.99721448, 2111), (0.41123188, 0.99721448, 2112), (0.41304348, 0.99721448, 2113), (0.41485507, 0.99721448, 2114), (0.41666667, 0.99721448, 2115), (0.41847826, 0.99721448, 2116), (0.42028986, 0.99721448, 2117), (0.42210145, 0.99721448, 2118), (0.42391304, 0.99721448, 2119), (0.42572464, 0.99721448, 2120), (0.42753623, 0.99721448, 2121), (0.42934783, 0.99721448, 2122), (0.43115942, 0.99721448, 2123), (0.43297101, 0.99721448, 2124), (0.43478261, 0.99721448, 2125), (0.4365942, 0.99721448, 2126), (0.4384058, 0.99721448, 2127), (0.44021739, 0.99721448, 2128), (0.44202899, 0.99721448, 2129), (0.44384058, 0.99721448, 2130), (0.44565217, 0.99721448, 2131), (0.44746377, 0.99721448, 2132), (0.44927536, 0.99721448, 2133), (0.45108696, 0.99721448, 2134), (0.45289855, 0.99721448, 2135), (0.45471014, 0.99721448, 2136), (0.45652174, 0.99721448, 2137), (0.45833333, 0.99721448, 2138), (0.46014493, 0.99721448, 2139), (0.46195652, 0.99721448, 2140), (0.46376812, 0.99721448, 2141), (0.46557971, 0.99721448, 2142), (0.4673913, 0.99721448, 2143), (0.4692029, 0.99721448, 2144), (0.47101449, 0.99721448, 2145), (0.47282609, 0.99721448, 2146), (0.47463768, 0.99721448, 2147), (0.47644928, 0.99721448, 2148), (0.47826087, 0.99721448, 2149), (0.48007246, 0.99721448, 2150), (0.48188406, 0.99721448, 2151), (0.48369565, 0.99721448, 2152), (0.48550725, 0.99721448, 2153), (0.48731884, 0.99721448, 2154), (0.48913043, 0.99721448, 2155), (0.49094203, 0.99721448, 2156), (0.49275362, 0.99721448, 2157), (0.49456522, 0.99721448, 2158), (0.49637681, 0.99721448, 2159), (0.49818841, 0.99721448, 2160), (0.5, 0.99721448, 2161), (0.50181159, 0.99721448, 2162), (0.50362319, 0.99721448, 2163), (0.50543478, 0.99721448, 2164), (0.50724638, 0.99721448, 2165), (0.50905797, 0.99721448, 2166), (0.51086957, 0.99721448, 2167), (0.51268116, 0.99721448, 2168), (0.51449275, 0.99721448, 2169), (0.51630435, 0.99721448, 2170), (0.51811594, 0.99721448, 2171), (0.51992754, 0.99721448, 2172), (0.52173913, 0.99721448, 2173), (0.52355072, 0.99721448, 2174), (0.52536232, 0.99721448, 2175), (0.52717391, 0.99721448, 2176), (0.52898551, 0.99721448, 2177), (0.5307971, 0.99721448, 2178), (0.5326087, 0.99721448, 2179), (0.53442029, 0.99721448, 2180), (0.53623188, 0.99721448, 2181), (0.53804348, 0.99721448, 2182), (0.53985507, 0.99721448, 2183), (0.54166667, 0.99721448, 2184), (0.54347826, 0.99721448, 2185), (0.54528986, 0.99721448, 2186), (0.54710145, 0.99721448, 2187), (0.54891304, 0.99721448, 2188), (0.55072464, 0.99721448, 2189), (0.55253623, 0.99721448, 2190), (0.55434783, 0.99721448, 2191), (0.55615942, 0.99721448, 2192), (0.55797101, 0.99721448, 2193), (0.55978261, 0.99721448, 2194), (0.5615942, 0.99721448, 2195), (0.5634058, 0.99721448, 2196), (0.56521739, 0.99721448, 2197), (0.56702899, 0.99721448, 2198), (0.56884058, 0.99721448, 2199), (0.57065217, 0.99721448, 2200), (0.57246377, 0.99721448, 2201), (0.57427536, 0.99721448, 2202), (0.57608696, 0.99721448, 2203), (0.57789855, 0.99721448, 2204), (0.57971014, 0.99721448, 2205), (0.58152174, 0.99721448, 2206), (0.58333333, 0.99721448, 2207), (0.58514493, 0.99721448, 2208), (0.58695652, 0.99721448, 2209), (0.58876812, 0.99721448, 2210), (0.59057971, 0.99721448, 2211), (0.5923913, 0.99721448, 2212), (0.5942029, 0.99721448, 2213), (0.59601449, 0.99721448, 2214), (0.59782609, 0.99721448, 2215), (0.59963768, 0.99721448, 2216), (0.60144928, 0.99721448, 2217), (0.60326087, 0.99721448, 2218), (0.60507246, 0.99721448, 2219), (0.60688406, 0.99721448, 2220), (0.60869565, 0.99721448, 2221), (0.61050725, 0.99721448, 2222), (0.61231884, 0.99721448, 2223), (0.61413043, 0.99721448, 2224), (0.61594203, 0.99721448, 2225), (0.61775362, 0.99721448, 2226), (0.61956522, 0.99721448, 2227), (0.62137681, 0.99721448, 2228), (0.62318841, 0.99721448, 2229), (0.625, 0.99721448, 2230), (0.62681159, 0.99721448, 2231), (0.62862319, 0.99721448, 2232), (0.63043478, 0.99721448, 2233), (0.63224638, 0.99721448, 2234), (0.63405797, 0.99721448, 2235), (0.63586957, 0.99721448, 2236), (0.63768116, 0.99721448, 2237), (0.63949275, 0.99721448, 2238), (0.64130435, 0.99721448, 2239), (0.64311594, 0.99721448, 2240), (0.64492754, 0.99721448, 2241), (0.64673913, 0.99721448, 2242), (0.64855072, 0.99721448, 2243), (0.65036232, 0.99721448, 2244), (0.65217391, 0.99721448, 2245), (0.65398551, 0.99721448, 2246), (0.6557971, 0.99721448, 2247), (0.6576087, 0.99721448, 2248), (0.65942029, 0.99721448, 2249), (0.66123188, 0.99721448, 2250), (0.66304348, 0.99721448, 2251), (0.66485507, 0.99721448, 2252), (0.66666667, 0.99721448, 2253), (0.66847826, 0.99721448, 2254), (0.67028986, 0.99721448, 2255), (0.67210145, 0.99721448, 2256), (0.67391304, 0.99721448, 2257), (0.67572464, 0.99721448, 2258), (0.67753623, 0.99721448, 2259), (0.67934783, 0.99721448, 2260), (0.68115942, 0.99721448, 2261), (0.68297101, 0.99721448, 2262), (0.68478261, 0.99721448, 2263), (0.6865942, 0.99721448, 2264), (0.6884058, 0.99721448, 2265), (0.69021739, 0.99721448, 2266), (0.69202899, 0.99721448, 2267), (0.69384058, 0.99721448, 2268), (0.69565217, 0.99721448, 2269), (0.69746377, 0.99721448, 2270), (0.69927536, 0.99721448, 2271), (0.70108696, 0.99721448, 2272), (0.70289855, 0.99721448, 2273), (0.70471014, 0.99721448, 2274), (0.70652174, 0.99721448, 2275), (0.70833333, 0.99721448, 2276), (0.71014493, 0.99721448, 2277), (0.71195652, 0.99721448, 2278), (0.71376812, 0.99721448, 2279), (0.71557971, 0.99721448, 2280), (0.7173913, 0.99721448, 2281), (0.7192029, 0.99721448, 2282), (0.72101449, 0.99721448, 2283), (0.72282609, 0.99721448, 2284), (0.72463768, 0.99721448, 2285), (0.72644928, 0.99721448, 2286), (0.72826087, 0.99721448, 2287), (0.73007246, 0.99721448, 2288), (0.73188406, 0.99721448, 2289), (0.73369565, 0.99721448, 2290), (0.73550725, 0.99721448, 2291), (0.73731884, 0.99721448, 2292), (0.73913043, 0.99721448, 2293), (0.74094203, 0.99721448, 2294), (0.74275362, 0.99721448, 2295), (0.74456522, 0.99721448, 2296), (0.74637681, 0.99721448, 2297), (0.74818841, 0.99721448, 2298), (0.75, 0.99721448, 2299), (0.75181159, 0.99721448, 2300), (0.75362319, 0.99721448, 2301), (0.75543478, 0.99721448, 2302), (0.75724638, 0.99721448, 2303), (0.75905797, 0.99721448, 2304), (0.76086957, 0.99721448, 2305), (0.76268116, 0.99721448, 2306), (0.76449275, 0.99721448, 2307), (0.76630435, 0.99721448, 2308), (0.76811594, 0.99721448, 2309), (0.76992754, 0.99721448, 2310), (0.77173913, 0.99721448, 2311), (0.77355072, 0.99721448, 2312), (0.77536232, 0.99721448, 2313), (0.77717391, 0.99721448, 2314), (0.77898551, 0.99721448, 2315), (0.7807971, 0.99721448, 2316), (0.7826087, 0.99721448, 2317), (0.78442029, 0.99721448, 2318), (0.78623188, 0.99721448, 2319), (0.78804348, 0.99721448, 2320), (0.78985507, 0.99721448, 2321), (0.79166667, 0.99721448, 2322), (0.79347826, 0.99721448, 2323), (0.79528986, 0.99721448, 2324), (0.79710145, 0.99721448, 2325), (0.79891304, 0.99721448, 2326), (0.80072464, 0.99721448, 2327), (0.80253623, 0.99721448, 2328), (0.80434783, 0.99721448, 2329), (0.80615942, 0.99721448, 2330), (0.80797101, 0.99721448, 2331), (0.80978261, 0.99721448, 2332), (0.8115942, 0.99721448, 2333), (0.8134058, 0.99721448, 2334), (0.81521739, 0.99721448, 2335), (0.81702899, 0.99721448, 2336), (0.81884058, 0.99721448, 2337), (0.82065217, 0.99721448, 2338), (0.82246377, 0.99721448, 2339), (0.82427536, 0.99721448, 2340), (0.82608696, 0.99721448, 2341), (0.82789855, 0.99721448, 2342), (0.82971014, 0.99721448, 2343), (0.83152174, 0.99721448, 2344), (0.83333333, 0.99721448, 2345), (0.83514493, 0.99721448, 2346), (0.83695652, 0.99721448, 2347), (0.83876812, 0.99721448, 2348), (0.84057971, 0.99721448, 2349), (0.8423913, 0.99721448, 2350), (0.8442029, 0.99721448, 2351), (0.84601449, 0.99721448, 2352), (0.84782609, 0.99721448, 2353), (0.84963768, 0.99721448, 2354), (0.85144928, 0.99721448, 2355), (0.85326087, 0.99721448, 2356), (0.85507246, 0.99721448, 2357), (0.85688406, 0.99721448, 2358), (0.85869565, 0.99721448, 2359), (0.86050725, 0.99721448, 2360), (0.86231884, 0.99721448, 2361), (0.86413043, 0.99721448, 2362), (0.86594203, 0.99721448, 2363), (0.86775362, 0.99721448, 2364), (0.86956522, 0.99721448, 2365), (0.87137681, 0.99721448, 2366), (0.87318841, 0.99721448, 2367), (0.875, 0.99721448, 2368), (0.87681159, 0.99721448, 2369), (0.87862319, 0.99721448, 2370), (0.88043478, 0.99721448, 2371), (0.88224638, 0.99721448, 2372), (0.88405797, 0.99721448, 2373), (0.88586957, 0.99721448, 2374), (0.88768116, 0.99721448, 2375), (0.88949275, 0.99721448, 2376), (0.89130435, 0.99721448, 2377), (0.89311594, 0.99721448, 2378), (0.89492754, 0.99721448, 2379), (0.89673913, 0.99721448, 2380), (0.89855072, 0.99721448, 2381), (0.90036232, 0.99721448, 2382), (0.90217391, 0.99721448, 2383), (0.90398551, 0.99721448, 2384), (0.9057971, 0.99721448, 2385), (0.9076087, 0.99721448, 2386), (0.90942029, 0.99721448, 2387), (0.91123188, 0.99721448, 2388), (0.91304348, 0.99721448, 2389), (0.91485507, 0.99721448, 2390), (0.91666667, 0.99721448, 2391), (0.91847826, 0.99721448, 2392), (0.92028986, 0.99721448, 2393), (0.92210145, 0.99721448, 2394), (0.92391304, 0.99721448, 2395), (0.92572464, 0.99721448, 2396), (0.92753623, 0.99721448, 2397), (0.92934783, 0.99721448, 2398), (0.93115942, 0.99721448, 2399), (0.93297101, 0.99721448, 2400), (0.93478261, 0.99721448, 2401), (0.9365942, 0.99721448, 2402), (0.9384058, 0.99721448, 2403), (0.94021739, 0.99721448, 2404), (0.94202899, 0.99721448, 2405), (0.94384058, 0.99721448, 2406), (0.94565217, 0.99721448, 2407), (0.94746377, 0.99721448, 2408), (0.94927536, 0.99721448, 2409), (0.95108696, 0.99721448, 2410), (0.95289855, 0.99721448, 2411), (0.95471014, 0.99721448, 2412), (0.95652174, 0.99721448, 2413), (0.95833333, 0.99721448, 2414), (0.96014493, 0.99721448, 2415), (0.96195652, 0.99721448, 2416), (0.96376812, 0.99721448, 2417), (0.96557971, 0.99721448, 2418), (0.9673913, 0.99721448, 2419), (0.9692029, 0.99721448, 2420), (0.97101449, 0.99721448, 2421), (0.97282609, 0.99721448, 2422), (0.97463768, 0.99721448, 2423), (0.97644928, 0.99721448, 2424), (0.97826087, 0.99721448, 2425), (0.98007246, 0.99721448, 2426), (0.98188406, 0.99721448, 2427), (0.98369565, 0.99721448, 2428), (0.98550725, 0.99721448, 2429), (0.98731884, 0.99721448, 2430), (0.98913043, 0.99721448, 2431), (0.99094203, 0.99721448, 2432), (0.99275362, 0.99721448, 2433), (0.99456522, 0.99721448, 2434), (0.99637681, 0.99721448, 2435), (0.99818841, 0.99721448, 2436), (0.99818841, 0.99442897, 2437), (0.99818841, 0.99164345, 2438), (0.99818841, 0.98885794, 2439), (0.99818841, 0.98607242, 2440), (0.99818841, 0.98328691, 2441), (0.99818841, 0.98050139, 2442), (0.99818841, 0.97771588, 2443), (0.99818841, 0.97493036, 2444), (0.99818841, 0.97214485, 2445), (0.99818841, 0.96935933, 2446), (0.99818841, 0.96657382, 2447), (0.99818841, 0.9637883, 2448), (0.99818841, 0.96100279, 2449), (0.99818841, 0.95821727, 2450), (0.99818841, 0.95543175, 2451), (0.99818841, 0.95264624, 2452), (0.99818841, 0.94986072, 2453), (0.99818841, 0.94707521, 2454), (0.99818841, 0.94428969, 2455), (0.99818841, 0.94150418, 2456), (0.99818841, 0.93871866, 2457), (0.99818841, 0.93593315, 2458), (0.99818841, 0.93314763, 2459), (0.99818841, 0.93036212, 2460), (0.99818841, 0.9275766, 2461), (0.99818841, 0.92479109, 2462), (0.99818841, 0.92200557, 2463), (0.99818841, 0.91922006, 2464), (0.99818841, 0.91643454, 2465), (0.99818841, 0.91364903, 2466), (0.99818841, 0.91086351, 2467), (0.99818841, 0.90807799, 2468), (0.99818841, 0.90529248, 2469), (0.99818841, 0.90250696, 2470), (0.99818841, 0.89972145, 2471), (0.99818841, 0.89693593, 2472), (0.99818841, 0.89415042, 2473), (0.99818841, 0.8913649, 2474), (0.99818841, 0.88857939, 2475), (0.99818841, 0.88579387, 2476), (0.99818841, 0.88300836, 2477), (0.99818841, 0.88022284, 2478), (0.99818841, 0.87743733, 2479), (0.99818841, 0.87465181, 2480), (0.99818841, 0.8718663, 2481), (0.99818841, 0.86908078, 2482), (0.99818841, 0.86629526, 2483), (0.99818841, 0.86350975, 2484), (0.99818841, 0.86072423, 2485), (0.99818841, 0.85793872, 2486), (0.99818841, 0.8551532, 2487), (0.99818841, 0.85236769, 2488), (0.99818841, 0.84958217, 2489), (0.99818841, 0.84679666, 2490), (0.99818841, 0.84401114, 2491), (0.99818841, 0.84122563, 2492), (0.99818841, 0.83844011, 2493), (0.99818841, 0.8356546, 2494), (0.99818841, 0.83286908, 2495), (0.99818841, 0.83008357, 2496), (0.99818841, 0.82729805, 2497), (0.99818841, 0.82451253, 2498), (0.99818841, 0.82172702, 2499), (0.99818841, 0.8189415, 2500), (0.99818841, 0.81615599, 2501), (0.99818841, 0.81337047, 2502), (0.99818841, 0.81058496, 2503), (0.99818841, 0.80779944, 2504), (0.99818841, 0.80501393, 2505), (0.99818841, 0.80222841, 2506), (0.99818841, 0.7994429, 2507), (0.99818841, 0.79665738, 2508), (0.99818841, 0.79387187, 2509), (0.99818841, 0.79108635, 2510), (0.99818841, 0.78830084, 2511), (0.99818841, 0.78551532, 2512), (0.99818841, 0.78272981, 2513), (0.99818841, 0.77994429, 2514), (0.99818841, 0.77715877, 2515), (0.99818841, 0.77437326, 2516), (0.99818841, 0.77158774, 2517), (0.99818841, 0.76880223, 2518), (0.99818841, 0.76601671, 2519), (0.99818841, 0.7632312, 2520), (0.99818841, 0.76044568, 2521), (0.99818841, 0.75766017, 2522), (0.99818841, 0.75487465, 2523), (0.99818841, 0.75208914, 2524), (0.99818841, 0.74930362, 2525), (0.99818841, 0.74651811, 2526), (0.99818841, 0.74373259, 2527), (0.99818841, 0.74094708, 2528), (0.99818841, 0.73816156, 2529), (0.99818841, 0.73537604, 2530), (0.99818841, 0.73259053, 2531), (0.99818841, 0.72980501, 2532), (0.99818841, 0.7270195, 2533), (0.99818841, 0.72423398, 2534), (0.99818841, 0.72144847, 2535), (0.99818841, 0.71866295, 2536), (0.99818841, 0.71587744, 2537), (0.99818841, 0.71309192, 2538), (0.99818841, 0.71030641, 2539), (0.99818841, 0.70752089, 2540), (0.99818841, 0.70473538, 2541), (0.99818841, 0.70194986, 2542), (0.99818841, 0.69916435, 2543), (0.99818841, 0.69637883, 2544), (0.99818841, 0.69359331, 2545), (0.99818841, 0.6908078, 2546), (0.99818841, 0.68802228, 2547), (0.99818841, 0.68523677, 2548), (0.99818841, 0.68245125, 2549), (0.99818841, 0.67966574, 2550), (0.99818841, 0.67688022, 2551), (0.99818841, 0.67409471, 2552), (0.99818841, 0.67130919, 2553), (0.99818841, 0.66852368, 2554), (0.99818841, 0.66573816, 2555), (0.99818841, 0.66295265, 2556), (0.99818841, 0.66016713, 2557), (0.99818841, 0.65738162, 2558), (0.99818841, 0.6545961, 2559), (0.99818841, 0.65181058, 2560), (0.99818841, 0.64902507, 2561), (0.99818841, 0.64623955, 2562), (0.99818841, 0.64345404, 2563), (0.99818841, 0.64066852, 2564), (0.99818841, 0.63788301, 2565), (0.99818841, 0.63509749, 2566), (0.99818841, 0.63231198, 2567), (0.99818841, 0.62952646, 2568), (0.99818841, 0.62674095, 2569), (0.99818841, 0.62395543, 2570), (0.99818841, 0.62116992, 2571), (0.99818841, 0.6183844, 2572), (0.99818841, 0.61559889, 2573), (0.99818841, 0.61281337, 2574), (0.99818841, 0.61002786, 2575), (0.99818841, 0.60724234, 2576), (0.99818841, 0.60445682, 2577), (0.99818841, 0.60167131, 2578), (0.99818841, 0.59888579, 2579), (0.99818841, 0.59610028, 2580), (0.99818841, 0.59331476, 2581), (0.99818841, 0.59052925, 2582), (0.99818841, 0.58774373, 2583), (0.99818841, 0.58495822, 2584), (0.99818841, 0.5821727, 2585), (0.99818841, 0.57938719, 2586), (0.99818841, 0.57660167, 2587), (0.99818841, 0.57381616, 2588), (0.99818841, 0.57103064, 2589), (0.99818841, 0.56824513, 2590), (0.99818841, 0.56545961, 2591), (0.99818841, 0.56267409, 2592), (0.99818841, 0.55988858, 2593), (0.99818841, 0.55710306, 2594), (0.99818841, 0.55431755, 2595), (0.99818841, 0.55153203, 2596), (0.99818841, 0.54874652, 2597), (0.99818841, 0.545961, 2598), (0.99818841, 0.54317549, 2599), (0.99818841, 0.54038997, 2600), (0.99818841, 0.53760446, 2601), (0.99818841, 0.53481894, 2602), (0.99818841, 0.53203343, 2603), (0.99818841, 0.52924791, 2604), (0.99818841, 0.5264624, 2605), (0.99818841, 0.52367688, 2606), (0.99818841, 0.52089136, 2607), (0.99818841, 0.51810585, 2608), (0.99818841, 0.51532033, 2609), (0.99818841, 0.51253482, 2610), (0.99818841, 0.5097493, 2611), (0.99818841, 0.50696379, 2612), (0.99818841, 0.50417827, 2613), (0.99818841, 0.50139276, 2614), (0.99818841, 0.49860724, 2615), (0.99818841, 0.49582173, 2616), (0.99818841, 0.49303621, 2617), (0.99818841, 0.4902507, 2618), (0.99818841, 0.48746518, 2619), (0.99818841, 0.48467967, 2620), (0.99818841, 0.48189415, 2621), (0.99818841, 0.47910864, 2622), (0.99818841, 0.47632312, 2623), (0.99818841, 0.4735376, 2624), (0.99818841, 0.47075209, 2625), (0.99818841, 0.46796657, 2626), (0.99818841, 0.46518106, 2627), (0.99818841, 0.46239554, 2628), (0.99818841, 0.45961003, 2629), (0.99818841, 0.45682451, 2630), (0.99818841, 0.454039, 2631), (0.99818841, 0.45125348, 2632), (0.99818841, 0.44846797, 2633), (0.99818841, 0.44568245, 2634), (0.99818841, 0.44289694, 2635), (0.99818841, 0.44011142, 2636), (0.99818841, 0.43732591, 2637), (0.99818841, 0.43454039, 2638), (0.99818841, 0.43175487, 2639), (0.99818841, 0.42896936, 2640), (0.99818841, 0.42618384, 2641), (0.99818841, 0.42339833, 2642), (0.99818841, 0.42061281, 2643), (0.99818841, 0.4178273, 2644), (0.99818841, 0.41504178, 2645), (0.99818841, 0.41225627, 2646), (0.99818841, 0.40947075, 2647), (0.99818841, 0.40668524, 2648), (0.99818841, 0.40389972, 2649), (0.99818841, 0.40111421, 2650), (0.99818841, 0.39832869, 2651), (0.99818841, 0.39554318, 2652), (0.99818841, 0.39275766, 2653), (0.99818841, 0.38997214, 2654), (0.99818841, 0.38718663, 2655), (0.99818841, 0.38440111, 2656), (0.99818841, 0.3816156, 2657), (0.99818841, 0.37883008, 2658), (0.99818841, 0.37604457, 2659), (0.99818841, 0.37325905, 2660), (0.99818841, 0.37047354, 2661), (0.99818841, 0.36768802, 2662), (0.99818841, 0.36490251, 2663), (0.99818841, 0.36211699, 2664), (0.99818841, 0.35933148, 2665), (0.99818841, 0.35654596, 2666), (0.99818841, 0.35376045, 2667), (0.99818841, 0.35097493, 2668), (0.99818841, 0.34818942, 2669), (0.99818841, 0.3454039, 2670), (0.99818841, 0.34261838, 2671), (0.99818841, 0.33983287, 2672), (0.99818841, 0.33704735, 2673), (0.99818841, 0.33426184, 2674), (0.99818841, 0.33147632, 2675), (0.99818841, 0.32869081, 2676), (0.99818841, 0.32590529, 2677), (0.99818841, 0.32311978, 2678), (0.99818841, 0.32033426, 2679), (0.99818841, 0.31754875, 2680), (0.99818841, 0.31476323, 2681), (0.99818841, 0.31197772, 2682), (0.99818841, 0.3091922, 2683), (0.99818841, 0.30640669, 2684), (0.99818841, 0.30362117, 2685), (0.99818841, 0.30083565, 2686), (0.99818841, 0.29805014, 2687), (0.99818841, 0.29526462, 2688), (0.99818841, 0.29247911, 2689), (0.99818841, 0.28969359, 2690), (0.99818841, 0.28690808, 2691), (0.99818841, 0.28412256, 2692), (0.99818841, 0.28133705, 2693), (0.99818841, 0.27855153, 2694), (0.99818841, 0.27576602, 2695), (0.99818841, 0.2729805, 2696), (0.99818841, 0.27019499, 2697), (0.99818841, 0.26740947, 2698), (0.99818841, 0.26462396, 2699), (0.99818841, 0.26183844, 2700), (0.99818841, 0.25905292, 2701), (0.99818841, 0.25626741, 2702), (0.99818841, 0.25348189, 2703), (0.99818841, 0.25069638, 2704), (0.99818841, 0.24791086, 2705), (0.99818841, 0.24512535, 2706), (0.99818841, 0.24233983, 2707), (0.99818841, 0.23955432, 2708), (0.99818841, 0.2367688, 2709), (0.99818841, 0.23398329, 2710), (0.99818841, 0.23119777, 2711), (0.99818841, 0.22841226, 2712), (0.99818841, 0.22562674, 2713), (0.99818841, 0.22284123, 2714), (0.99818841, 0.22005571, 2715), (0.99818841, 0.21727019, 2716), (0.99818841, 0.21448468, 2717), (0.99818841, 0.21169916, 2718), (0.99818841, 0.20891365, 2719), (0.99818841, 0.20612813, 2720), (0.99818841, 0.20334262, 2721), (0.99818841, 0.2005571, 2722), (0.99818841, 0.19777159, 2723), (0.99818841, 0.19498607, 2724), (0.99818841, 0.19220056, 2725), (0.99818841, 0.18941504, 2726), (0.99818841, 0.18662953, 2727), (0.99818841, 0.18384401, 2728), (0.99818841, 0.1810585, 2729), (0.99818841, 0.17827298, 2730), (0.99818841, 0.17548747, 2731), (0.99818841, 0.17270195, 2732), (0.99818841, 0.16991643, 2733), (0.99818841, 0.16713092, 2734), (0.99818841, 0.1643454, 2735), (0.99818841, 0.16155989, 2736), (0.99818841, 0.15877437, 2737), (0.99818841, 0.15598886, 2738), (0.99818841, 0.15320334, 2739), (0.99818841, 0.15041783, 2740), (0.99818841, 0.14763231, 2741), (0.99818841, 0.1448468, 2742), (0.99818841, 0.14206128, 2743), (0.99818841, 0.13927577, 2744), (0.99818841, 0.13649025, 2745), (0.99818841, 0.13370474, 2746), (0.99818841, 0.13091922, 2747), (0.99818841, 0.1281337, 2748), (0.99818841, 0.12534819, 2749), (0.99818841, 0.12256267, 2750), (0.99818841, 0.11977716, 2751), (0.99818841, 0.11699164, 2752), (0.99818841, 0.11420613, 2753), (0.99818841, 0.11142061, 2754), (0.99818841, 0.1086351, 2755), (0.99818841, 0.10584958, 2756), (0.99818841, 0.10306407, 2757), (0.99818841, 0.10027855, 2758), (0.99818841, 0.09749304, 2759), (0.99818841, 0.09470752, 2760), (0.99818841, 0.09192201, 2761), (0.99818841, 0.08913649, 2762), (0.99818841, 0.08635097, 2763), (0.99818841, 0.08356546, 2764), (0.99818841, 0.08077994, 2765), (0.99818841, 0.07799443, 2766), (0.99818841, 0.07520891, 2767), (0.99818841, 0.0724234, 2768), (0.99818841, 0.06963788, 2769), (0.99818841, 0.06685237, 2770), (0.99818841, 0.06406685, 2771), (0.99818841, 0.06128134, 2772), (0.99818841, 0.05849582, 2773), (0.99818841, 0.05571031, 2774), (0.99818841, 0.05292479, 2775), (0.99818841, 0.05013928, 2776), (0.99818841, 0.04735376, 2777), (0.99818841, 0.04456825, 2778), (0.99818841, 0.04178273, 2779), (0.99818841, 0.03899721, 2780), (0.99818841, 0.0362117, 2781), (0.99818841, 0.03342618, 2782), (0.99818841, 0.03064067, 2783), (0.99818841, 0.02785515, 2784), (0.99818841, 0.02506964, 2785), (0.99818841, 0.02228412, 2786), (0.99818841, 0.01949861, 2787), (0.99818841, 0.01671309, 2788), (0.99818841, 0.01392758, 2789), (0.99818841, 0.01114206, 2790), (0.99818841, 0.00835655, 2791), (0.99818841, 0.00557103, 2792), (0.99818841, 0.00278552, 2793), (0.99818841, 0.0, 2794), (0.99637681, 0.0, 2795), (0.99456522, 0.0, 2796), (0.99275362, 0.0, 2797), (0.99094203, 0.0, 2798), (0.98913043, 0.0, 2799), (0.98731884, 0.0, 2800), (0.98550725, 0.0, 2801), (0.98369565, 0.0, 2802), (0.98188406, 0.0, 2803), (0.98007246, 0.0, 2804), (0.97826087, 0.0, 2805), (0.97644928, 0.0, 2806), (0.97463768, 0.0, 2807), (0.97282609, 0.0, 2808), (0.97101449, 0.0, 2809), (0.9692029, 0.0, 2810), (0.9673913, 0.0, 2811), (0.96557971, 0.0, 2812), (0.96376812, 0.0, 2813), (0.96195652, 0.0, 2814), (0.96014493, 0.0, 2815), (0.95833333, 0.0, 2816), (0.95652174, 0.0, 2817), (0.95471014, 0.0, 2818), (0.95289855, 0.0, 2819), (0.95108696, 0.0, 2820), (0.94927536, 0.0, 2821), (0.94746377, 0.0, 2822), (0.94565217, 0.0, 2823), (0.94384058, 0.0, 2824), (0.94202899, 0.0, 2825), (0.94021739, 0.0, 2826), (0.9384058, 0.0, 2827), (0.9365942, 0.0, 2828), (0.93478261, 0.0, 2829), (0.93297101, 0.0, 2830), (0.93115942, 0.0, 2831), (0.92934783, 0.0, 2832), (0.92753623, 0.0, 2833), (0.92572464, 0.0, 2834), (0.92391304, 0.0, 2835), (0.92210145, 0.0, 2836), (0.92028986, 0.0, 2837), (0.91847826, 0.0, 2838), (0.91666667, 0.0, 2839), (0.91485507, 0.0, 2840), (0.91304348, 0.0, 2841), (0.91123188, 0.0, 2842), (0.90942029, 0.0, 2843), (0.9076087, 0.0, 2844), (0.9057971, 0.0, 2845), (0.90398551, 0.0, 2846), (0.90217391, 0.0, 2847), (0.90036232, 0.0, 2848), (0.89855072, 0.0, 2849), (0.89673913, 0.0, 2850), (0.89492754, 0.0, 2851), (0.89311594, 0.0, 2852), (0.89130435, 0.0, 2853), (0.88949275, 0.0, 2854), (0.88768116, 0.0, 2855), (0.88586957, 0.0, 2856), (0.88405797, 0.0, 2857), (0.88224638, 0.0, 2858), (0.88043478, 0.0, 2859), (0.87862319, 0.0, 2860), (0.87681159, 0.0, 2861), (0.875, 0.0, 2862), (0.87318841, 0.0, 2863), (0.87137681, 0.0, 2864), (0.86956522, 0.0, 2865), (0.86775362, 0.0, 2866), (0.86594203, 0.0, 2867), (0.86413043, 0.0, 2868), (0.86231884, 0.0, 2869), (0.86050725, 0.0, 2870), (0.85869565, 0.0, 2871), (0.85688406, 0.0, 2872), (0.85507246, 0.0, 2873), (0.85326087, 0.0, 2874), (0.85144928, 0.0, 2875), (0.84963768, 0.0, 2876), (0.84782609, 0.0, 2877), (0.84601449, 0.0, 2878), (0.8442029, 0.0, 2879), (0.8423913, 0.0, 2880), (0.84057971, 0.0, 2881), (0.83876812, 0.0, 2882), (0.83695652, 0.0, 2883), (0.83514493, 0.0, 2884), (0.83333333, 0.0, 2885), (0.83152174, 0.0, 2886), (0.82971014, 0.0, 2887), (0.82789855, 0.0, 2888), (0.82608696, 0.0, 2889), (0.82427536, 0.0, 2890), (0.82246377, 0.0, 2891), (0.82065217, 0.0, 2892), (0.81884058, 0.0, 2893), (0.81702899, 0.0, 2894), (0.81521739, 0.0, 2895), (0.8134058, 0.0, 2896), (0.8115942, 0.0, 2897), (0.80978261, 0.0, 2898), (0.80797101, 0.0, 2899), (0.80615942, 0.0, 2900), (0.80434783, 0.0, 2901), (0.80253623, 0.0, 2902), (0.80072464, 0.0, 2903), (0.79891304, 0.0, 2904), (0.79710145, 0.0, 2905), (0.79528986, 0.0, 2906), (0.79347826, 0.0, 2907), (0.79166667, 0.0, 2908), (0.78985507, 0.0, 2909), (0.78804348, 0.0, 2910), (0.78623188, 0.0, 2911), (0.78442029, 0.0, 2912), (0.7826087, 0.0, 2913), (0.7807971, 0.0, 2914), (0.77898551, 0.0, 2915), (0.77717391, 0.0, 2916), (0.77536232, 0.0, 2917), (0.77355072, 0.0, 2918), (0.77173913, 0.0, 2919), (0.76992754, 0.0, 2920), (0.76811594, 0.0, 2921), (0.76630435, 0.0, 2922), (0.76449275, 0.0, 2923), (0.76268116, 0.0, 2924), (0.76086957, 0.0, 2925), (0.75905797, 0.0, 2926), (0.75724638, 0.0, 2927), (0.75543478, 0.0, 2928), (0.75362319, 0.0, 2929), (0.75181159, 0.0, 2930), (0.75, 0.0, 2931), (0.74818841, 0.0, 2932), (0.74637681, 0.0, 2933), (0.74456522, 0.0, 2934), (0.74275362, 0.0, 2935), (0.74094203, 0.0, 2936), (0.73913043, 0.0, 2937), (0.73731884, 0.0, 2938), (0.73550725, 0.0, 2939), (0.73369565, 0.0, 2940), (0.73188406, 0.0, 2941), (0.73007246, 0.0, 2942), (0.72826087, 0.0, 2943), (0.72644928, 0.0, 2944), (0.72463768, 0.0, 2945), (0.72282609, 0.0, 2946), (0.72101449, 0.0, 2947), (0.7192029, 0.0, 2948), (0.7173913, 0.0, 2949), (0.71557971, 0.0, 2950), (0.71376812, 0.0, 2951), (0.71195652, 0.0, 2952), (0.71014493, 0.0, 2953), (0.70833333, 0.0, 2954), (0.70652174, 0.0, 2955), (0.70471014, 0.0, 2956), (0.70289855, 0.0, 2957), (0.70108696, 0.0, 2958), (0.69927536, 0.0, 2959), (0.69746377, 0.0, 2960), (0.69565217, 0.0, 2961), (0.69384058, 0.0, 2962), (0.69202899, 0.0, 2963), (0.69021739, 0.0, 2964), (0.6884058, 0.0, 2965), (0.6865942, 0.0, 2966), (0.68478261, 0.0, 2967), (0.68297101, 0.0, 2968), (0.68115942, 0.0, 2969), (0.67934783, 0.0, 2970), (0.67753623, 0.0, 2971), (0.67572464, 0.0, 2972), (0.67391304, 0.0, 2973), (0.67210145, 0.0, 2974), (0.67028986, 0.0, 2975), (0.66847826, 0.0, 2976), (0.66666667, 0.0, 2977), (0.66485507, 0.0, 2978), (0.66304348, 0.0, 2979), (0.66123188, 0.0, 2980), (0.65942029, 0.0, 2981), (0.6576087, 0.0, 2982), (0.6557971, 0.0, 2983), (0.65398551, 0.0, 2984), (0.65217391, 0.0, 2985), (0.65036232, 0.0, 2986), (0.64855072, 0.0, 2987), (0.64673913, 0.0, 2988), (0.64492754, 0.0, 2989), (0.64311594, 0.0, 2990), (0.64130435, 0.0, 2991), (0.63949275, 0.0, 2992), (0.63768116, 0.0, 2993), (0.63586957, 0.0, 2994), (0.63405797, 0.0, 2995), (0.63224638, 0.0, 2996), (0.63043478, 0.0, 2997), (0.62862319, 0.0, 2998), (0.62681159, 0.0, 2999), (0.625, 0.0, 3000), (0.62318841, 0.0, 3001), (0.62137681, 0.0, 3002), (0.61956522, 0.0, 3003), (0.61775362, 0.0, 3004), (0.61594203, 0.0, 3005), (0.61413043, 0.0, 3006), (0.61231884, 0.0, 3007), (0.61050725, 0.0, 3008), (0.60869565, 0.0, 3009), (0.60688406, 0.0, 3010), (0.60507246, 0.0, 3011), (0.60326087, 0.0, 3012), (0.60144928, 0.0, 3013), (0.59963768, 0.0, 3014), (0.59782609, 0.0, 3015), (0.59601449, 0.0, 3016), (0.5942029, 0.0, 3017), (0.5923913, 0.0, 3018), (0.59057971, 0.0, 3019), (0.58876812, 0.0, 3020), (0.58695652, 0.0, 3021), (0.58514493, 0.0, 3022), (0.58333333, 0.0, 3023), (0.58152174, 0.0, 3024), (0.57971014, 0.0, 3025), (0.57789855, 0.0, 3026), (0.57608696, 0.0, 3027), (0.57427536, 0.0, 3028), (0.57246377, 0.0, 3029), (0.57065217, 0.0, 3030), (0.56884058, 0.0, 3031), (0.56702899, 0.0, 3032), (0.56521739, 0.0, 3033), (0.5634058, 0.0, 3034), (0.5615942, 0.0, 3035), (0.55978261, 0.0, 3036), (0.55797101, 0.0, 3037), (0.55615942, 0.0, 3038), (0.55434783, 0.0, 3039), (0.55253623, 0.0, 3040), (0.55072464, 0.0, 3041), (0.54891304, 0.0, 3042), (0.54710145, 0.0, 3043), (0.54528986, 0.0, 3044), (0.54347826, 0.0, 3045), (0.54166667, 0.0, 3046), (0.53985507, 0.0, 3047), (0.53804348, 0.0, 3048), (0.53623188, 0.0, 3049), (0.53442029, 0.0, 3050), (0.5326087, 0.0, 3051), (0.5307971, 0.0, 3052), (0.52898551, 0.0, 3053), (0.52717391, 0.0, 3054), (0.52536232, 0.0, 3055), (0.52355072, 0.0, 3056), (0.52173913, 0.0, 3057), (0.51992754, 0.0, 3058), (0.51811594, 0.0, 3059), (0.51630435, 0.0, 3060), (0.51449275, 0.0, 3061), (0.51268116, 0.0, 3062), (0.51086957, 0.0, 3063), (0.50905797, 0.0, 3064), (0.50724638, 0.0, 3065), (0.50543478, 0.0, 3066), (0.50362319, 0.0, 3067), (0.50181159, 0.0, 3068), (0.5, 0.0, 3069), (0.49818841, 0.0, 3070), (0.49637681, 0.0, 3071), (0.49456522, 0.0, 3072), (0.49275362, 0.0, 3073), (0.49094203, 0.0, 3074), (0.48913043, 0.0, 3075), (0.48731884, 0.0, 3076), (0.48550725, 0.0, 3077), (0.48369565, 0.0, 3078), (0.48188406, 0.0, 3079), (0.48007246, 0.0, 3080), (0.47826087, 0.0, 3081), (0.47644928, 0.0, 3082), (0.47463768, 0.0, 3083), (0.47282609, 0.0, 3084), (0.47101449, 0.0, 3085), (0.4692029, 0.0, 3086), (0.4673913, 0.0, 3087), (0.46557971, 0.0, 3088), (0.46376812, 0.0, 3089), (0.46195652, 0.0, 3090), (0.46014493, 0.0, 3091), (0.45833333, 0.0, 3092), (0.45652174, 0.0, 3093), (0.45471014, 0.0, 3094), (0.45289855, 0.0, 3095), (0.45108696, 0.0, 3096), (0.44927536, 0.0, 3097), (0.44746377, 0.0, 3098), (0.44565217, 0.0, 3099), (0.44384058, 0.0, 3100), (0.44202899, 0.0, 3101), (0.44021739, 0.0, 3102), (0.4384058, 0.0, 3103), (0.4365942, 0.0, 3104), (0.43478261, 0.0, 3105), (0.43297101, 0.0, 3106), (0.43115942, 0.0, 3107), (0.42934783, 0.0, 3108), (0.42753623, 0.0, 3109), (0.42572464, 0.0, 3110), (0.42391304, 0.0, 3111), (0.42210145, 0.0, 3112), (0.42028986, 0.0, 3113), (0.41847826, 0.0, 3114), (0.41666667, 0.0, 3115), (0.41485507, 0.0, 3116), (0.41304348, 0.0, 3117), (0.41123188, 0.0, 3118), (0.40942029, 0.0, 3119), (0.4076087, 0.0, 3120), (0.4057971, 0.0, 3121), (0.40398551, 0.0, 3122), (0.40217391, 0.0, 3123), (0.40036232, 0.0, 3124), (0.39855072, 0.0, 3125), (0.39673913, 0.0, 3126), (0.39492754, 0.0, 3127), (0.39311594, 0.0, 3128), (0.39130435, 0.0, 3129), (0.38949275, 0.0, 3130), (0.38768116, 0.0, 3131), (0.38586957, 0.0, 3132), (0.38405797, 0.0, 3133), (0.38224638, 0.0, 3134), (0.38043478, 0.0, 3135), (0.37862319, 0.0, 3136), (0.37681159, 0.0, 3137), (0.375, 0.0, 3138), (0.37318841, 0.0, 3139), (0.37137681, 0.0, 3140), (0.36956522, 0.0, 3141), (0.36775362, 0.0, 3142), (0.36594203, 0.0, 3143), (0.36413043, 0.0, 3144), (0.36231884, 0.0, 3145), (0.36050725, 0.0, 3146), (0.35869565, 0.0, 3147), (0.35688406, 0.0, 3148), (0.35507246, 0.0, 3149), (0.35326087, 0.0, 3150), (0.35144928, 0.0, 3151), (0.34963768, 0.0, 3152), (0.34782609, 0.0, 3153), (0.34601449, 0.0, 3154), (0.3442029, 0.0, 3155), (0.3423913, 0.0, 3156), (0.34057971, 0.0, 3157), (0.33876812, 0.0, 3158), (0.33695652, 0.0, 3159), (0.33514493, 0.0, 3160), (0.33333333, 0.0, 3161), (0.33152174, 0.0, 3162), (0.32971014, 0.0, 3163), (0.32789855, 0.0, 3164), (0.32608696, 0.0, 3165), (0.32427536, 0.0, 3166), (0.32246377, 0.0, 3167), (0.32065217, 0.0, 3168), (0.31884058, 0.0, 3169), (0.31702899, 0.0, 3170), (0.31521739, 0.0, 3171), (0.3134058, 0.0, 3172), (0.3115942, 0.0, 3173), (0.30978261, 0.0, 3174), (0.30797101, 0.0, 3175), (0.30615942, 0.0, 3176), (0.30434783, 0.0, 3177), (0.30253623, 0.0, 3178), (0.30072464, 0.0, 3179), (0.29891304, 0.0, 3180), (0.29710145, 0.0, 3181), (0.29528986, 0.0, 3182), (0.29347826, 0.0, 3183), (0.29166667, 0.0, 3184), (0.28985507, 0.0, 3185), (0.28804348, 0.0, 3186), (0.28623188, 0.0, 3187), (0.28442029, 0.0, 3188), (0.2826087, 0.0, 3189), (0.2807971, 0.0, 3190), (0.27898551, 0.0, 3191), (0.27717391, 0.0, 3192), (0.27536232, 0.0, 3193), (0.27355072, 0.0, 3194), (0.27173913, 0.0, 3195), (0.26992754, 0.0, 3196), (0.26811594, 0.0, 3197), (0.26630435, 0.0, 3198), (0.26449275, 0.0, 3199), (0.26268116, 0.0, 3200), (0.26086957, 0.0, 3201), (0.25905797, 0.0, 3202), (0.25724638, 0.0, 3203), (0.25543478, 0.0, 3204), (0.25362319, 0.0, 3205), (0.25181159, 0.0, 3206), (0.25, 0.0, 3207), (0.24818841, 0.0, 3208), (0.24637681, 0.0, 3209), (0.24456522, 0.0, 3210), (0.24275362, 0.0, 3211), (0.24094203, 0.0, 3212), (0.23913043, 0.0, 3213), (0.23731884, 0.0, 3214), (0.23550725, 0.0, 3215), (0.23369565, 0.0, 3216), (0.23188406, 0.0, 3217), (0.23007246, 0.0, 3218), (0.22826087, 0.0, 3219), (0.22644928, 0.0, 3220), (0.22463768, 0.0, 3221), (0.22282609, 0.0, 3222), (0.22101449, 0.0, 3223), (0.2192029, 0.0, 3224), (0.2173913, 0.0, 3225), (0.21557971, 0.0, 3226), (0.21376812, 0.0, 3227), (0.21195652, 0.0, 3228), (0.21014493, 0.0, 3229), (0.20833333, 0.0, 3230), (0.20652174, 0.0, 3231), (0.20471014, 0.0, 3232), (0.20289855, 0.0, 3233), (0.20108696, 0.0, 3234), (0.19927536, 0.0, 3235), (0.19746377, 0.0, 3236), (0.19565217, 0.0, 3237), (0.19384058, 0.0, 3238), (0.19202899, 0.0, 3239), (0.19021739, 0.0, 3240), (0.1884058, 0.0, 3241), (0.1865942, 0.0, 3242), (0.18478261, 0.0, 3243), (0.18297101, 0.0, 3244), (0.18115942, 0.0, 3245), (0.17934783, 0.0, 3246), (0.17753623, 0.0, 3247), (0.17572464, 0.0, 3248), (0.17391304, 0.0, 3249), (0.17210145, 0.0, 3250), (0.17028986, 0.0, 3251), (0.16847826, 0.0, 3252), (0.16666667, 0.0, 3253), (0.16485507, 0.0, 3254), (0.16304348, 0.0, 3255), (0.16123188, 0.0, 3256), (0.15942029, 0.0, 3257), (0.1576087, 0.0, 3258), (0.1557971, 0.0, 3259), (0.15398551, 0.0, 3260), (0.15217391, 0.0, 3261), (0.15036232, 0.0, 3262), (0.14855072, 0.0, 3263), (0.14673913, 0.0, 3264), (0.14492754, 0.0, 3265), (0.14311594, 0.0, 3266), (0.14130435, 0.0, 3267), (0.13949275, 0.0, 3268), (0.13768116, 0.0, 3269), (0.13586957, 0.0, 3270), (0.13405797, 0.0, 3271), (0.13224638, 0.0, 3272), (0.13043478, 0.0, 3273), (0.12862319, 0.0, 3274), (0.12681159, 0.0, 3275), (0.125, 0.0, 3276), (0.12318841, 0.0, 3277), (0.12137681, 0.0, 3278), (0.11956522, 0.0, 3279), (0.11775362, 0.0, 3280), (0.11594203, 0.0, 3281), (0.11413043, 0.0, 3282), (0.11231884, 0.0, 3283), (0.11050725, 0.0, 3284), (0.10869565, 0.0, 3285), (0.10688406, 0.0, 3286), (0.10507246, 0.0, 3287), (0.10326087, 0.0, 3288), (0.10144928, 0.0, 3289), (0.09963768, 0.0, 3290), (0.09782609, 0.0, 3291), (0.09601449, 0.0, 3292), (0.0942029, 0.0, 3293), (0.0923913, 0.0, 3294), (0.09057971, 0.0, 3295), (0.08876812, 0.0, 3296), (0.08695652, 0.0, 3297), (0.08514493, 0.0, 3298), (0.08333333, 0.0, 3299), (0.08152174, 0.0, 3300), (0.07971014, 0.0, 3301), (0.07789855, 0.0, 3302), (0.07608696, 0.0, 3303), (0.07427536, 0.0, 3304), (0.07246377, 0.0, 3305), (0.07065217, 0.0, 3306), (0.06884058, 0.0, 3307), (0.06702899, 0.0, 3308), (0.06521739, 0.0, 3309), (0.0634058, 0.0, 3310), (0.0615942, 0.0, 3311), (0.05978261, 0.0, 3312), (0.05797101, 0.0, 3313), (0.05615942, 0.0, 3314), (0.05434783, 0.0, 3315), (0.05253623, 0.0, 3316), (0.05072464, 0.0, 3317), (0.04891304, 0.0, 3318), (0.04710145, 0.0, 3319), (0.04528986, 0.0, 3320), (0.04347826, 0.0, 3321), (0.04166667, 0.0, 3322), (0.03985507, 0.0, 3323), (0.03804348, 0.0, 3324), (0.03623188, 0.0, 3325), (0.03442029, 0.0, 3326), (0.0326087, 0.0, 3327), (0.0307971, 0.0, 3328), (0.02898551, 0.0, 3329), (0.02717391, 0.0, 3330), (0.02536232, 0.0, 3331), (0.02355072, 0.0, 3332), (0.02173913, 0.0, 3333), (0.01992754, 0.0, 3334), (0.01811594, 0.0, 3335), (0.01630435, 0.0, 3336), (0.01449275, 0.0, 3337), (0.01268116, 0.0, 3338), (0.01086957, 0.0, 3339), (0.00905797, 0.0, 3340), (0.00724638, 0.0, 3341), (0.00543478, 0.0, 3342), (0.00362319, 0.0, 3343), (0.00181159, 0.0, 3344)]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_path = \"C:/Users/HP/Pictures/test.png\"  # Replace with your image path\n",
    "strokes = extract_strokes_with_timestamps(image_path)\n",
    "print(strokes)  # Output: [(x1, y1, t1)\n",
    "# strokes_to_inkml(strokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_extracted_strokes(strokes, target_seq_length=110, char2idx=None):\n",
    "    \"\"\"\n",
    "    Process extracted strokes from an image to match the format used during training.\n",
    "    \n",
    "    Args:\n",
    "        strokes: List of strokes where each stroke is a list of (x, y, timestamp) tuples\n",
    "        target_seq_length: The fixed sequence length expected by the model\n",
    "        char2idx: Character to index mapping (only needed for labels)\n",
    "        \n",
    "    Returns:\n",
    "        Processed strokes in the expected model input format\n",
    "    \"\"\"\n",
    "    # Flatten strokes into a single sequence of (x, y, timestamp) points\n",
    "    flattened_strokes = []\n",
    "    for stroke in strokes:\n",
    "        flattened_strokes.extend(stroke)\n",
    "    \n",
    "    # Convert to numpy array and extract features\n",
    "    stroke_array = np.array(flattened_strokes)\n",
    "    \n",
    "    # If the stroke array is empty, create a default one\n",
    "    if len(stroke_array) == 0:\n",
    "        stroke_array = np.zeros((1, 3))\n",
    "    \n",
    "    # Take only the first target_seq_length points or pad if needed\n",
    "    if len(stroke_array) > target_seq_length:\n",
    "        processed_strokes = stroke_array[:target_seq_length]\n",
    "    else:\n",
    "        # Pad with zeros to reach target_seq_length\n",
    "        padding = np.zeros((target_seq_length - len(stroke_array), 3))\n",
    "        processed_strokes = np.vstack([stroke_array, padding])\n",
    "    \n",
    "    # Add the extra feature dimension to match your training format\n",
    "    processed_strokes = np.pad(processed_strokes, ((0, 0), (0, 1)), mode='constant')\n",
    "    \n",
    "    # Reshape to match the expected batch input shape\n",
    "    processed_strokes = np.expand_dims(processed_strokes, axis=0)\n",
    "    \n",
    "    return processed_strokes\n",
    "\n",
    "# Example usage with your extracted strokes\n",
    "def predict_from_image(image_path, model, idx2char, target_seq_length=110, max_time_steps=128):\n",
    "    \"\"\"\n",
    "    Extract strokes from an image and make a prediction using your trained model\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image\n",
    "        model: Your trained model\n",
    "        char2idx: Character to index mapping\n",
    "        idx2char: Index to character mapping\n",
    "        target_seq_length: The fixed sequence length expected by the model\n",
    "        max_time_steps: Maximum time steps for decoder input\n",
    "        \n",
    "    Returns:\n",
    "        Predicted text\n",
    "    \"\"\"\n",
    "    # Extract strokes from image\n",
    "    strokes = extract_strokes_with_timestamps(image_path)\n",
    "    \n",
    "    # Preprocess the strokes to match model's expected input format\n",
    "    processed_strokes = preprocess_extracted_strokes(strokes, target_seq_length)\n",
    "    \n",
    "    # Create decoder input (assuming you're using the same approach as in training)\n",
    "    start_token = 0\n",
    "    decoder_input = np.array([[start_token]])  # Initial token to start decoding\n",
    "    \n",
    "    # Create padding to match MAX_TIME_STEPS if necessary\n",
    "    if max_time_steps > 1:\n",
    "        padding = np.zeros((1, max_time_steps - 1))\n",
    "        decoder_input = np.hstack([decoder_input, padding])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict([processed_strokes, decoder_input])\n",
    "    \n",
    "    # Convert prediction to text\n",
    "    # This depends on your model output format, but typically:\n",
    "    predicted_indices = np.argmax(prediction, axis=-1)[0]\n",
    "    \n",
    "    # Stop at end token or padding\n",
    "    text = \"\"\n",
    "    for idx in predicted_indices:\n",
    "        if idx == 0 or idx not in idx2char:  # Assuming 0 is padding or end token\n",
    "            break\n",
    "        text += idx2char[idx]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_extracted_strokes(strokes, target_seq_length=110):\n",
    "    \"\"\"\n",
    "    Process extracted strokes from an image to match the format used during training.\n",
    "    \n",
    "    Args:\n",
    "        strokes: List of strokes where each stroke is a list of (x, y, timestamp) tuples\n",
    "        target_seq_length: The fixed sequence length expected by the model\n",
    "        \n",
    "    Returns:\n",
    "        Processed strokes in the expected model input format\n",
    "    \"\"\"\n",
    "    # Flatten strokes into a single sequence of (x, y, timestamp) points\n",
    "    flattened_strokes = []\n",
    "    for stroke in strokes:\n",
    "        flattened_strokes.extend(stroke)\n",
    "    \n",
    "    # Convert to numpy array and extract features\n",
    "    if len(flattened_strokes) == 0:\n",
    "        # Handle empty strokes\n",
    "        stroke_array = np.zeros((1, 3))\n",
    "    else:\n",
    "        stroke_array = np.array(flattened_strokes)\n",
    "    \n",
    "    # Extract x, y coordinates and timestamps\n",
    "    # Normalize coordinates if needed (similar to training preprocessing)\n",
    "    # You might need to adjust this based on your original preprocessing\n",
    "    \n",
    "    # Take only the first target_seq_length points or pad if needed\n",
    "    if len(stroke_array) > target_seq_length:\n",
    "        processed_strokes = stroke_array[:target_seq_length, :]\n",
    "    else:\n",
    "        # Pad with zeros to reach target_seq_length\n",
    "        padding = np.zeros((target_seq_length - len(stroke_array), 3))\n",
    "        processed_strokes = np.vstack([stroke_array, padding])\n",
    "    \n",
    "    # Important: Create shape (batch_size, seq_length, 3) \n",
    "    # The error says model expects shape=(None, None, 3), so no need for extra feature dimension\n",
    "    processed_strokes = processed_strokes[:, :3]  # Ensure we only have 3 features\n",
    "    \n",
    "    # Reshape to match the expected batch input shape\n",
    "    processed_strokes = np.expand_dims(processed_strokes, axis=0)\n",
    "    \n",
    "    return processed_strokes\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict_from_image(image_path, model, idx2char, target_seq_length=110, max_time_steps=128):\n",
    "    \"\"\"\n",
    "    Extract strokes from an image and make a prediction using your trained model.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image\n",
    "        model: Your trained model\n",
    "        idx2char: Index-to-character mapping\n",
    "        target_seq_length: The fixed sequence length expected by the model\n",
    "        max_time_steps: Maximum time steps for decoder input\n",
    "        \n",
    "    Returns:\n",
    "        Predicted text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract strokes from the image\n",
    "    strokes = extract_strokes_with_timestamps(image_path)\n",
    "    \n",
    "    # Preprocess the strokes to match model's expected input format\n",
    "    processed_strokes = preprocess_extracted_strokes(strokes, target_seq_length)\n",
    "    \n",
    "    # Print shapes for debugging\n",
    "    print(f\"Processed strokes shape: {processed_strokes.shape}\")\n",
    "    \n",
    "    # Create decoder input with correct initialization\n",
    "    start_token = 0\n",
    "    decoder_input = np.full((1, max_time_steps), start_token, dtype=np.int32)  # Use np.full() for proper initialization\n",
    "    \n",
    "    print(f\"Decoder input shape: {decoder_input.shape}\")\n",
    "    \n",
    "    # Make prediction\n",
    "    try:\n",
    "        prediction = model.predict([processed_strokes, decoder_input])\n",
    "        print(f\"Prediction shape: {prediction.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        # Debug model input shapes\n",
    "        for i, inp in enumerate(model.inputs):\n",
    "            print(f\"Model input {i} shape: {inp.shape}\")\n",
    "        raise\n",
    "    \n",
    "    return prediction\n",
    "    \n",
    "    # # Convert prediction to text\n",
    "    # predicted_indices = np.argmax(prediction, axis=-1)[0]\n",
    "    \n",
    "    # # Stop at end token or padding\n",
    "    # text = \"\"\n",
    "    # for idx in predicted_indices:\n",
    "    #     if idx == 0 or idx not in idx2char:  # Assuming 0 is padding or end token\n",
    "    #         break\n",
    "    #     text += idx2char[idx]\n",
    "    \n",
    "    # return text\n",
    "\n",
    "def full_prediction_workflow(image_path, model,idx2char):\n",
    "    \"\"\"\n",
    "    Complete workflow from image to text prediction\n",
    "    \"\"\"\n",
    "    # Extract input shapes from model for debugging\n",
    "    print(\"Model expects:\")\n",
    "    for i, inp in enumerate(model.inputs):\n",
    "        print(f\"Input {i} shape: {inp.shape}\")\n",
    "    \n",
    "    # Perform the prediction\n",
    "    try:\n",
    "        prediction = predict_from_image(image_path, model,idx2char=idx2char)\n",
    "        print(f\"Predicted text: {prediction}\")\n",
    "        return prediction\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction workflow: {e}\")\n",
    "        \n",
    "        # # Let's try a more flexible approach by creating inputs based on model's expected shapes\n",
    "        # try:\n",
    "        #     print(\"Attempting alternative approach...\")\n",
    "        #     # Create dummy inputs with shapes that match the model's expectations\n",
    "        #     inputs = []\n",
    "        #     for i, inp in enumerate(model.inputs):\n",
    "        #         shape = [1 if dim is None else dim for dim in inp.shape.as_list()]\n",
    "        #         if i == 0:  # Assume first input is strokes\n",
    "        #             # Extract strokes and reshape according to expected dimensions\n",
    "        #             strokes = extract_strokes_with_timestamps(image_path)\n",
    "        #             processed = preprocess_extracted_strokes(strokes)\n",
    "        #             # Reshape if necessary\n",
    "        #             if len(shape) > 1 and shape[1] is None:\n",
    "        #                 shape[1] = processed.shape[1]\n",
    "        #             inputs.append(processed.reshape(shape))\n",
    "        #         else:\n",
    "        #             # For other inputs (like decoder inputs), create zeros with expected shape\n",
    "        #             dummy_input = np.zeros(shape)\n",
    "        #             if i == 1:  # Assume second input is decoder input\n",
    "        #                 dummy_input[0, 0] = 0  # Start token\n",
    "        #             inputs.append(dummy_input)\n",
    "        #         print(f\"Created input {i} with shape {inputs[-1].shape}\")\n",
    "            \n",
    "        #     # Try prediction with these inputs\n",
    "        #     prediction = model.predict(inputs)\n",
    "        #     print(f\"Alternative prediction shape: {prediction.shape}\")\n",
    "            \n",
    "        # #     # Handle result\n",
    "        # #     idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "        # #     predicted_indices = np.argmax(prediction, axis=-1)[0]\n",
    "        # #     text = \"\"\n",
    "        # #     for idx in predicted_indices:\n",
    "        # #         if idx == 0 or idx not in idx2char:\n",
    "        # #             break\n",
    "        # #         text += idx2char[idx]\n",
    "        # #     print(f\"Alternative predicted text: {text}\")\n",
    "        # #     return text\n",
    "        # except Exception as e2:\n",
    "        #     print(f\"Alternative approach failed: {e2}\")\n",
    "        #     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dictionary: {'1': ' ', '2': '!', '3': '#', '4': '%', '5': '&', '6': '(', '7': ')', '8': '*', '9': '+', '10': ',', '11': '-', '12': '.', '13': '/', '14': '0', '15': '1', '16': '2', '17': '3', '18': '4', '19': '5', '20': '6', '21': '7', '22': '8', '23': '9', '24': ':', '25': ';', '26': '<', '27': '=', '28': '>', '29': '?', '30': 'A', '31': 'B', '32': 'C', '33': 'D', '34': 'E', '35': 'F', '36': 'G', '37': 'H', '38': 'I', '39': 'J', '40': 'K', '41': 'L', '42': 'M', '43': 'N', '44': 'O', '45': 'P', '46': 'Q', '47': 'R', '48': 'S', '49': 'T', '50': 'U', '51': 'V', '52': 'W', '53': 'X', '54': 'Y', '55': 'Z', '56': '[', '57': '\\\\', '58': ']', '59': '^', '60': '_', '61': 'a', '62': 'b', '63': 'c', '64': 'd', '65': 'e', '66': 'f', '67': 'g', '68': 'h', '69': 'i', '70': 'j', '71': 'k', '72': 'l', '73': 'm', '74': 'n', '75': 'o', '76': 'p', '77': 'q', '78': 'r', '79': 's', '80': 't', '81': 'u', '82': 'v', '83': 'w', '84': 'x', '85': 'y', '86': 'z', '87': '{', '88': '|', '89': '}', '0': '<pad>'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"EXP1_char2idx.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    loaded_dict = json.load(file)\n",
    "\n",
    "print(\"Loaded Dictionary:\", loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects:\n",
      "Input 0 shape: (None, None, 3)\n",
      "Input 1 shape: (None, None)\n",
      "Processed strokes shape: (1, 110, 3)\n",
      "Decoder input shape: (1, 128)\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025FD13A0B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Prediction shape: (1, 128, 100)\n",
      "Predicted text: [[[0.00611791 0.02234418 0.00490657 ... 0.00455681 0.00505696 0.00514279]\n",
      "  [0.00611791 0.02234418 0.00490657 ... 0.00455681 0.00505696 0.00514279]\n",
      "  [0.00611791 0.02234418 0.00490657 ... 0.00455681 0.00505696 0.00514279]\n",
      "  ...\n",
      "  [0.00611791 0.02234418 0.00490657 ... 0.00455681 0.00505696 0.00514279]\n",
      "  [0.00611791 0.02234418 0.00490657 ... 0.00455681 0.00505696 0.00514279]\n",
      "  [0.00611791 0.02234418 0.00490657 ... 0.00455681 0.00505696 0.00514279]]]\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "image_path = \"C:/Users/HP/Pictures/test.png\"\n",
    "prediction = full_prediction_workflow(image_path, model2, loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n",
      "  57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n",
      "  57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n",
      "  57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n",
      "  57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57\n",
      "  57 57 57 57 57 57 57 57]]\n"
     ]
    }
   ],
   "source": [
    "predicted_token_ids = np.argmax(prediction, axis=-1) \n",
    "print(predicted_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions_to_latex(predictions, idx2char):\n",
    "    \"\"\"\n",
    "    Convert a sequence of predicted token IDs into a LaTeX equation.\n",
    "\n",
    "    Args:\n",
    "        predictions (list or np.array): List of predicted token IDs.\n",
    "        idx2char (dict): Dictionary mapping token IDs to LaTeX tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: LaTeX formatted equation.\n",
    "    \"\"\"\n",
    "    latex_tokens = [idx2char.get(token, \"\") for token in predictions if token in idx2char]\n",
    "    latex_expression = \" \".join(latex_tokens)  # Join tokens with space for readability\n",
    "    return f\"\\\\[{latex_expression}\\\\]\"  # Wrap with LaTeX math mode delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m latex_equation \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_predictions_to_latex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_token_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx2char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloaded_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m latex_equation\n",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m, in \u001b[0;36mconvert_predictions_to_latex\u001b[1;34m(predictions, idx2char)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_predictions_to_latex\u001b[39m(predictions, idx2char):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Convert a sequence of predicted token IDs into a LaTeX equation.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m        str: LaTeX formatted equation.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     latex_tokens \u001b[38;5;241m=\u001b[39m [idx2char\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m predictions \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m idx2char]\n\u001b[0;32m     13\u001b[0m     latex_expression \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(latex_tokens)  \u001b[38;5;66;03m# Join tokens with space for readability\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatex_expression\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_predictions_to_latex\u001b[39m(predictions, idx2char):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Convert a sequence of predicted token IDs into a LaTeX equation.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m        str: LaTeX formatted equation.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     latex_tokens \u001b[38;5;241m=\u001b[39m [idx2char\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m predictions \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx2char\u001b[49m]\n\u001b[0;32m     13\u001b[0m     latex_expression \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(latex_tokens)  \u001b[38;5;66;03m# Join tokens with space for readability\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatex_expression\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "latex_equation = convert_predictions_to_latex(predicted_token_ids, idx2char=loaded_dict)\n",
    "latex_equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eduboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
